{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EADDqQ4hJvEM","Z-reuIsGln39","WrkJY_GJKMIW","efbPxR1nPHlp","eAW3j_J3KZcE","0nVyDpEDP0DM"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**What it is:**  \n","- This script generates benchmark associations for real estate (REIT) portfolios by mapping portfolio names to appropriate benchmarks defined in the BenchmarkGeneralInformation (BGI) table.\n","\n","**What it does:**  \n","- Retrieves active REIT portfolios from `PortfolioGeneralInformation`.  \n","- Retrieves benchmark metadata from `BenchmarkGeneralInformation`.  \n","- Uses keyword matching between portfolio `NAME` and benchmark `NAME` to determine relevant associations.  \n","- Creates a ranked list of associations and prepares it for insertion into the `PortfolioBenchmarkAssociation` table in Snowflake.\n","\n","**Why it’s important:**  \n","- Ensures that each REIT portfolio is linked to one or more benchmarks, enabling comparative performance analysis.  \n","- Supports automation and scalability by reducing reliance on hard-coded rules.  \n","- Maintains data integrity across analytics and reporting pipelines by programmatically populating key relationships.\n"],"metadata":{"id":"kqVPGkumm8H8"}},{"cell_type":"markdown","source":["# **Portfolio Benchmark Association Table**"],"metadata":{"id":"EADDqQ4hJvEM"}},{"cell_type":"markdown","source":["## Establishing Connection"],"metadata":{"id":"Z-reuIsGln39"}},{"cell_type":"markdown","source":["You must download the env.txt file which is located in my GDrive folder (\"Shiv-Code\") and upload using the \"Choose Files\" widget after running the below cell."],"metadata":{"id":"aCaqWuKnfCMD"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"oOwM03pkkw8y","executionInfo":{"status":"ok","timestamp":1754084097691,"user_tz":240,"elapsed":6396,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"41f3ae6c-a51d-44be-8257-4c072aefbeab"},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-46e43863-b0a6-45a6-8c92-eac4042226d6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-46e43863-b0a6-45a6-8c92-eac4042226d6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving env.txt to env.txt\n"]}]},{"cell_type":"code","source":["import os\n","\n","if os.path.exists(\"env.txt\"):\n","    os.rename(\"env.txt\", \"env\")\n","    print(\"Renamed env.txt to env\")\n","else:\n","    print(\"File not found. Make sure you uploaded env.txt.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOoYQafKk7vQ","executionInfo":{"status":"ok","timestamp":1754084099136,"user_tz":240,"elapsed":6,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"2c0db6b0-8e45-4f22-e9ad-337617455855"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Renamed env.txt to env\n"]}]},{"cell_type":"code","source":["!pip install snowflake-connector-python python-dotenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPbMHknlk_L7","executionInfo":{"status":"ok","timestamp":1754084117140,"user_tz":240,"elapsed":16914,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"59025704-1444-4992-8bea-6e67dc742170"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting snowflake-connector-python\n","  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n","  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting boto3>=1.24 (from snowflake-connector-python)\n","  Downloading boto3-1.40.1-py3-none-any.whl.metadata (6.7 kB)\n","Collecting botocore>=1.24 (from snowflake-connector-python)\n","  Downloading botocore-1.40.1-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n","Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n","Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n","Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (25.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.7.14)\n","Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n","Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n","Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n","Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n","Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python)\n","  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n","Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.40.1-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.40.1-py3-none-any.whl (13.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: asn1crypto, python-dotenv, jmespath, botocore, s3transfer, boto3, snowflake-connector-python\n","Successfully installed asn1crypto-1.5.1 boto3-1.40.1 botocore-1.40.1 jmespath-1.0.1 python-dotenv-1.1.1 s3transfer-0.13.1 snowflake-connector-python-3.16.0\n"]}]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(\"env\")  # This loads the Snowflake credentials into the environment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-e_ija5ylBIe","executionInfo":{"status":"ok","timestamp":1754084117160,"user_tz":240,"elapsed":19,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"c5d433d0-c910-42f1-c4a0-8df4c74fbbe1"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import snowflake.connector\n","import os\n","\n","# Pull in credentials\n","conn = snowflake.connector.connect(\n","    user=os.getenv(\"SNOWFLAKE_USER\"),\n","    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n","    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n","    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n","    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n","    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n","    schema=os.getenv(\"SNOWFLAKE_SCHEMA\")\n",")\n","\n","# Run a simple test query\n","cur = conn.cursor()\n","cur.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE();\")\n","for row in cur:\n","    print(row)\n","\n","cur.close()\n","conn.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80emU-fllD84","executionInfo":{"status":"ok","timestamp":1754084121544,"user_tz":240,"elapsed":4383,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"23053be0-bbb8-445a-fee8-e8428e7d4170"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["('SHIVNEETN', 'AST_REALESTATE_DB_RW', 'AST_REALESTATE_DB')\n"]}]},{"cell_type":"markdown","source":["## Define Portfolio Code to Benchmark Code Mapping Logic"],"metadata":{"id":"WrkJY_GJKMIW"}},{"cell_type":"code","source":["\"\"\"\n","Auto-generates benchmark associations for REIT portfolios by matching\n","keywords in portfolio names with benchmark names from BGI.\n","\"\"\"\n","\n","import pandas as pd\n","from snowflake.connector import connect\n","import os\n","\n","# Connect to Snowflake\n","conn = connect(\n","    user=os.getenv(\"SNOWFLAKE_USER\"),\n","    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n","    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n","    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n","    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n","    database=\"AST_REALESTATE_DB\",\n","    schema=\"DBO\"\n",")\n","\n","# Step 1: Pull active REIT portfolios from PGI\n","\"\"\"\n","Fetches active REIT portfolios from PortfolioGeneralInformation.\n","The NAME column is assumed to embed strategy details for matching.\n","\"\"\"\n","pgi_df = pd.read_sql(\"\"\"\n","    SELECT PORTFOLIOCODE, NAME\n","    FROM PortfolioGeneralInformation\n","    WHERE PORTFOLIOCATEGORY = 'REIT'\n","      AND TERMINATIONDATE IS NULL\n","\"\"\", conn)\n","\n","# Step 2: Pull REIT benchmark names from BGI\n","\"\"\"\n","Fetches REIT-related benchmarks from BenchmarkGeneralInformation,\n","including the BENCHMARKCODE and full NAME for keyword comparisons.\n","\"\"\"\n","bgi_df = pd.read_sql(\"\"\"\n","    SELECT BENCHMARKCODE, NAME AS BENCHMARKNAME\n","    FROM BenchmarkGeneralInformation\n","    WHERE BENCHMARKCODE LIKE 'REITBENCH%'\n","\"\"\", conn)\n","\n","# Step 3: Attempt to match portfolio name keywords to benchmark names\n","\"\"\"\n","Loops through each portfolio name and attempts to find benchmark name matches.\n","\n","- First tries to match the full portfolio name against benchmark names.\n","- If no matches are found, falls back to word-level partial matching.\n","- Produces an association row for each match with a calculated rank.\n","\"\"\"\n","rows = []\n","for _, p_row in pgi_df.iterrows():\n","    portfolio_code = p_row[\"PORTFOLIOCODE\"]\n","    portfolio_name = p_row[\"NAME\"]\n","\n","    # Attempt full-name match\n","    matched_benchmarks = bgi_df[\n","        bgi_df[\"BENCHMARKNAME\"].str.contains(portfolio_name, case=False, na=False)\n","    ]\n","\n","    # Fallback: word-level match from portfolio name\n","    if matched_benchmarks.empty:\n","        words = portfolio_name.split()\n","        matched_benchmarks = bgi_df[\n","            bgi_df[\"BENCHMARKNAME\"].apply(\n","                lambda x: any(word.lower() in x.lower() for word in words)\n","            )\n","        ]\n","\n","    if matched_benchmarks.empty:\n","        print(f\"No benchmark match found for portfolio name '{portfolio_name}' ({portfolio_code})\")\n","        continue\n","\n","    for rank, (_, b_row) in enumerate(matched_benchmarks.iterrows(), start=1):\n","        rows.append({\n","            \"PORTFOLIOCODE\": portfolio_code,\n","            \"BENCHMARKCODE\": b_row[\"BENCHMARKCODE\"],\n","            \"RANK\": rank,\n","            \"RECIPIENTCODE\": None\n","        })\n","\n","# Step 4: Final association DataFrame\n","\"\"\"\n","Builds the final association DataFrame (assoc_df)\n","from all matches, ready for downstream Snowflake upload.\n","\"\"\"\n","assoc_df = pd.DataFrame(rows)\n","print(f\"Prepared {len(assoc_df)} benchmark associations based on portfolio NAME matching.\")\n","print(assoc_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OO12msNzlPzr","executionInfo":{"status":"ok","timestamp":1754084646768,"user_tz":240,"elapsed":1433,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"9294281d-2ce4-43fe-e890-13eeb51e802c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2764563708.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n","  pgi_df = pd.read_sql(\"\"\"\n","/tmp/ipython-input-2764563708.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n","  bgi_df = pd.read_sql(\"\"\"\n"]},{"output_type":"stream","name":"stdout","text":["No benchmark match found for portfolio name 'Industrial, Retail & Infrastructure' (REITPORTFOLIO005)\n","Prepared 97 benchmark associations based on portfolio NAME matching.\n","      PORTFOLIOCODE BENCHMARKCODE  RANK RECIPIENTCODE\n","0   REITPORTFOLIO02   REITBENCH01     1          None\n","1   REITPORTFOLIO02   REITBENCH02     2          None\n","2   REITPORTFOLIO18   REITBENCH01     1          None\n","3   REITPORTFOLIO18   REITBENCH02     2          None\n","4   REITPORTFOLIO13   REITBENCH01     1          None\n","..              ...           ...   ...           ...\n","92      PORTFOLIO03   REITBENCH02     2          None\n","93      PORTFOLIO02   REITBENCH01     1          None\n","94      PORTFOLIO02   REITBENCH02     2          None\n","95      PORTFOLIO01   REITBENCH01     1          None\n","96      PORTFOLIO01   REITBENCH02     2          None\n","\n","[97 rows x 4 columns]\n"]}]},{"cell_type":"markdown","source":["## Validation"],"metadata":{"id":"efbPxR1nPHlp"}},{"cell_type":"code","source":["import logging\n","import pandas as pd\n","\n","\n","def validate_portfolio_benchmark_association(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Validate the PortfolioBenchmarkAssociation data before uploading to Snowflake.\n","\n","    Checks include:\n","    - Required column presence\n","    - Nulls in key fields (PORTFOLIOCODE, BENCHMARKCODE, RANK)\n","    - Duplicate associations\n","    - RANK values being valid integers >= 1\n","\n","    Parameters\n","    ----------\n","    df : pd.DataFrame\n","        DataFrame containing portfolio-benchmark associations.\n","\n","    Returns\n","    -------\n","    pd.DataFrame\n","        One-row summary DataFrame with validation flags and issue counts.\n","    \"\"\"\n","    required_cols = [\"PORTFOLIOCODE\", \"BENCHMARKCODE\", \"RANK\", \"RECIPIENTCODE\"]\n","    missing_cols = [col for col in required_cols if col not in df.columns]\n","\n","    if missing_cols:\n","        logging.error(f\"[portfolio_benchmark_association] Missing required columns: {missing_cols}\")\n","        return pd.DataFrame()\n","\n","    # Validation summary\n","    summary = {\n","        \"missing_portfoliocodes\": df[\"PORTFOLIOCODE\"].isnull().sum(),\n","        \"missing_benchmarkcodes\": df[\"BENCHMARKCODE\"].isnull().sum(),\n","        \"missing_ranks\": df[\"RANK\"].isnull().sum(),\n","        \"invalid_rank_values\": df[\"RANK\"].apply(lambda r: not isinstance(r, (int, float)) or r < 1).sum(),\n","        \"duplicate_pairs\": df.duplicated(subset=[\"PORTFOLIOCODE\", \"BENCHMARKCODE\"]).sum()\n","    }\n","\n","    # Logging\n","    for key, count in summary.items():\n","        if count > 0:\n","            logging.warning(f\"[portfolio_benchmark_association] {key.replace('_', ' ').capitalize()}: {count}\")\n","\n","    if not any(summary.values()):\n","        logging.info(\"[portfolio_benchmark_association] Validation passed.\")\n","\n","    return pd.DataFrame([summary])\n"],"metadata":{"id":"ktbanLSnPJNT","executionInfo":{"status":"ok","timestamp":1754084676262,"user_tz":240,"elapsed":3,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Populate PBA Table in Snowflake"],"metadata":{"id":"eAW3j_J3KZcE"}},{"cell_type":"code","source":["\"\"\"\n","Creates a staging table, uploads benchmark associations, and merges into the\n","PortfolioBenchmarkAssociation table in Snowflake.\n","\"\"\"\n","\n","import os\n","import pandas as pd\n","from snowflake.connector import connect\n","from snowflake.connector.pandas_tools import write_pandas\n","\n","# Connect to Snowflake\n","conn = connect(\n","    user=os.getenv(\"SNOWFLAKE_USER\"),\n","    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n","    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n","    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n","    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n","    database=\"AST_REALESTATE_DB\",\n","    schema=\"DBO\"\n",")\n","\n","# Step 1: Set database and schema context, and create staging table\n","with conn.cursor() as cur:\n","    \"\"\"\n","    Establishes the context to the correct database/schema,\n","    and creates (or replaces) a temporary staging table to hold\n","    portfolio-benchmark associations before merging into the target table.\n","    \"\"\"\n","    cur.execute(\"USE DATABASE AST_REALESTATE_DB\")\n","    cur.execute(\"USE SCHEMA DBO\")\n","\n","    cur.execute(\"\"\"\n","        CREATE OR REPLACE TEMP TABLE STG_PORTFOLIOBENCHMARKASSOCIATION (\n","            PORTFOLIOCODE VARCHAR,\n","            BENCHMARKCODE VARCHAR,\n","            RANK NUMBER,\n","            RECIPIENTCODE VARCHAR\n","        )\n","    \"\"\")\n","\n","# Step 2: Upload assoc_df to staging table\n","\"\"\"\n","Uploads the DataFrame `assoc_df` to the staging table in Snowflake\n","using Snowflake's optimized `write_pandas` function.\n","\"\"\"\n","success, nchunks, nrows, _ = write_pandas(\n","    conn=conn,\n","    df=assoc_df,\n","    table_name=\"STG_PORTFOLIOBENCHMARKASSOCIATION\",\n","    quote_identifiers=False\n",")\n","print(f\"Uploaded {nrows} rows to STG_PORTFOLIOBENCHMARKASSOCIATION\")\n","\n","# Step 3: Merge staging data into target table\n","with conn.cursor() as cur:\n","    \"\"\"\n","    Merges data from the staging table into the target\n","    `PortfolioBenchmarkAssociation` table.\n","\n","    - If a (PORTFOLIOCODE, BENCHMARKCODE) pair exists, it updates RANK and RECIPIENTCODE.\n","    - Otherwise, it inserts a new row.\n","    \"\"\"\n","    cur.execute(\"\"\"\n","        MERGE INTO PortfolioBenchmarkAssociation AS target\n","        USING STG_PORTFOLIOBENCHMARKASSOCIATION AS source\n","        ON target.PORTFOLIOCODE = source.PORTFOLIOCODE\n","           AND target.BENCHMARKCODE = source.BENCHMARKCODE\n","\n","        WHEN MATCHED THEN UPDATE SET\n","            target.RANK = source.RANK,\n","            target.RECIPIENTCODE = source.RECIPIENTCODE\n","\n","        WHEN NOT MATCHED THEN INSERT (\n","            PORTFOLIOCODE, BENCHMARKCODE, RANK, RECIPIENTCODE\n","        ) VALUES (\n","            source.PORTFOLIOCODE, source.BENCHMARKCODE, source.RANK, source.RECIPIENTCODE\n","        )\n","    \"\"\")\n","    print(\"MERGE into PortfolioBenchmarkAssociation completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Cu0TFkiKcaw","executionInfo":{"status":"ok","timestamp":1754084754452,"user_tz":240,"elapsed":5182,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"d94d159a-1e24-4adb-811f-01793e780bc0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Uploaded 97 rows to STG_PORTFOLIOBENCHMARKASSOCIATION\n","MERGE into PortfolioBenchmarkAssociation completed.\n"]}]},{"cell_type":"markdown","source":["# **Data Flow & Architecture Mapping**"],"metadata":{"id":"0nVyDpEDP0DM"}},{"cell_type":"markdown","source":["## Source to Storage Mapping"],"metadata":{"id":"80-s_aGkP9CR"}},{"cell_type":"markdown","source":["| Stage                 | Description                                                                                      |\n","| --------------------- | ------------------------------------------------------------------------------------------------ |\n","| **Source**            | `PortfolioGeneralInformation` table in Snowflake (`PORTFOLIOCODE`, `NAME`, etc.)                 |\n","| **Extraction**        | PGI data is queried using a `SELECT` on Snowflake; loaded into Python as `pgi_df`                |\n","| **Transformation**    | Benchmark mapping logic parses `NAME` using keyword rules to assign benchmark(s) with ranking    |\n","|  • `get_benchmarks()` | Uses keyword-based rule matching to select appropriate benchmark(s) for each portfolio           |\n","|  • `enumerate()`      | Assigns `RANK` (1 = primary, 2 = secondary, etc.) based on order of matching benchmarks          |\n","| **Validation**        | `validate_portfolio_benchmark_association()` flags missing values, invalid ranks, and duplicates |\n","| **Storage**           | Enriched `assoc_df` is uploaded via `write_pandas()` into staging, then `MERGE`d into `PBA`      |\n","| **Consumption**       | Used by benchmark performance lookups, strategy comparables, reporting dashboards, and analytics |\n"],"metadata":{"id":"YKhg4QOGQCEn"}},{"cell_type":"markdown","source":["## Data Flow Diagram"],"metadata":{"id":"5PKOuYbZQFjI"}},{"cell_type":"markdown","source":["          +-----------------------------+\n","          | PortfolioGeneralInformation|\n","          |  (Snowflake source)        |\n","          +-----------------------------+\n","                      |\n","            [SELECT * WHERE PORTFOLIOCATEGORY = 'REIT']\n","                      |\n","                      v\n","         +------------------------------------+\n","         | get_benchmarks(portfolio_name)     |  ← Keyword rule-matching logic\n","         | → returns benchmark list           |\n","         +------------------------------------+\n","                      |\n","                      v\n","         +------------------------------------+\n","         | Build assoc_df                     |  ← Loop: assign RANK and RECIPIENTCODE\n","         +------------------------------------+\n","                      |\n","                      v\n","         +------------------------------------+\n","         | validate_portfolio_benchmark_association()  |\n","         | → Checks nulls, types, duplicates           |\n","         +------------------------------------+\n","                      |\n","                      v\n","         +------------------------------------+\n","         | write_pandas() → STG_PBA (temp)    |\n","         +------------------------------------+\n","                      |\n","                      v\n","         +------------------------------------+\n","         | MERGE into PortfolioBenchmarkAssociation |\n","         +------------------------------------+\n","                      |\n","                      v\n","         +------------------------------------+\n","         | Assette UI / Reporting / Analytics |\n","         +------------------------------------+\n"],"metadata":{"id":"n-vIwdloQHbz"}},{"cell_type":"markdown","source":["## Transformation Logic Documentation"],"metadata":{"id":"JKiLDomwQLAU"}},{"cell_type":"markdown","source":["### Benchmark Mapping"],"metadata":{"id":"fu1wne1ZQQC6"}},{"cell_type":"markdown","source":["| Trigger Keyword | Benchmarks Assigned            | Notes                                          |\n","| --------------- | ------------------------------ | ---------------------------------------------- |\n","| `\"Dividend\"`    | `REITBENCH003`, `REITBENCH001` | Income-focused portfolios                      |\n","| `\"Utilities\"`   | `REITBENCH003`                 | Defensive sector exposure                      |\n","| `\"Tech\"`        | `REITBENCH002`                 | Growth-tilted real estate (e.g., data centers) |\n","| `\"Growth\"`      | `REITBENCH002`                 | Same as above                                  |\n","| `\"Diversified\"` | `REITBENCH001`, `REITBENCH003` | Broad exposure                                 |\n","| `\"Healthcare\"`  | `REITBENCH003`                 | Traditional REIT-focused exposure              |\n","| `\"Insurance\"`   | `REITBENCH003`                 | Similar sector alignment                       |\n","| `\"Industrial\"`  | `REITBENCH001`                 | Aligned with industrial REIT representation    |\n","| `\"Retail\"`      | `REITBENCH001`                 | Aligned with traditional retail REITs          |\n"],"metadata":{"id":"Jk51hdb2QMSj"}},{"cell_type":"markdown","source":["- Benchmark assignment is non-exclusive — portfolios may receive multiple benchmarks\n","\n","- RANK is determined by the order of keyword rule matches"],"metadata":{"id":"69WuVktzQWLm"}},{"cell_type":"markdown","source":["### Validation"],"metadata":{"id":"cp-GontrQbeB"}},{"cell_type":"markdown","source":["| Check                  | Description                                                            |\n","| ---------------------- | ---------------------------------------------------------------------- |\n","| Required columns       | Must include `PORTFOLIOCODE`, `BENCHMARKCODE`, `RANK`, `RECIPIENTCODE` |\n","| Missing values         | Flags nulls in key fields                                              |\n","| Rank sanity            | Ensures `RANK` is ≥1 and a valid integer                               |\n","| Duplicate combinations | Flags duplicate `(PORTFOLIOCODE, BENCHMARKCODE)` entries               |\n","| Logging                | Uses Python `logging.warning()` for diagnostics                        |\n"],"metadata":{"id":"MRcdxUK6Qc75"}},{"cell_type":"markdown","source":["## Future Enhancements"],"metadata":{"id":"mcwctz_CQf6i"}},{"cell_type":"markdown","source":["| Feature                | Description                                                           |\n","| ---------------------- | --------------------------------------------------------------------- |\n","| Dynamic Rule Config    | Extract benchmark rules into external YAML or DB table for governance |\n","| Strategy Tag Join      | Use PGI tags or strategy labels for more precise matching             |\n","| Manual Overrides       | Allow human-curated benchmark matches via override lookup table       |\n","| Backfill Audit History | Track `INGESTED_TIMESTAMP`, `ASSIGNED_BY_RULE`, and rule version used |\n","| Scheduled Refresh      | Add Snowflake Task or cron-based Python job for daily sync            |\n","| UI Benchmark Preview   | Display benchmark assignments in admin dashboard pre-upload           |\n"],"metadata":{"id":"FsIgBfQOQh1s"}}]}