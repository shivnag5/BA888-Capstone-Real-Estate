{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["iqcMeWOTgGCU","QhR6vUAUVf4E","McFU7dQb1F2c","vdf8Fyiu19F4","DOkFJ-SE2Qci","QXyftd5m2ub0","TC3T8LSF3Phj","AztM9Zg6ktLu","Z-reuIsGln39","vrBWriaN_cVd","PlHOb50oLu69"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**What it is**:  \n","  - This end-to-end script defines, builds, and loads structured portfolio metadata (Portfolio General Information) for a REIT-focused investment system. It includes portfolio names, styles, inception details, and currency configuration.\n","\n","**What it does**:  \n","  - **Defines portfolios** statically in a Python dictionary with fields like name, investment style, and product code  \n","  - **Builds the PGI DataFrame (`pgi_df`)**, assigning REIT-specific metadata, open dates, and currency information  \n","  - **Formats and cleans** the date columns to ensure compatibility with Snowflake  \n","  - **Connects to Snowflake** and creates a temporary staging table (`STG_PGI`)  \n","  - **Uploads the data** using `write_pandas()`  \n","  - **Merges into the main `PortfolioGeneralInformation` table**, ensuring existing portfolios are updated and new ones are inserted\n","\n","**Why it's important**:  \n","  - Provides a single source of truth for portfolio metadata  \n","  - Supports consistent data modeling across the platform  \n","  - Enables accurate linkage with benchmarks, performance tracking, and downstream analytics  \n","  - Designed to be extendable for new portfolios or evolving business requirements\n"],"metadata":{"id":"ZswEpDfAqy8Y"}},{"cell_type":"markdown","source":["# **Portfolio General Information Table**"],"metadata":{"id":"iqcMeWOTgGCU"}},{"cell_type":"markdown","source":["## Populating PGI Table and mapping portfolio codes {FINAL}"],"metadata":{"id":"QhR6vUAUVf4E"}},{"cell_type":"code","source":["import requests, pandas as pd, time\n","from bs4 import BeautifulSoup\n","from typing import List\n","\n","# Custom user-agent for web requests\n","UA = {\"User-Agent\": \"Mozilla/5.0 (MSBA Capstone Bot/1.0)\"}\n","\n","def load_sec_master() -> pd.DataFrame:\n","    \"\"\"\n","    Loads ticker and CIK mapping from the SEC's JSON feed.\n","\n","    Returns:\n","        DataFrame with standardized 'Ticker' and zero-padded 'CIK'.\n","    \"\"\"\n","    url = \"https://www.sec.gov/files/company_tickers_exchange.json\"\n","    data = requests.get(url, headers=UA, timeout=30).json()\n","    fields = data[\"fields\"]\n","    df = pd.DataFrame(data[\"data\"], columns=fields)\n","    df[\"Ticker\"] = df[\"ticker\"].str.upper()\n","    df[\"CIK\"] = df[\"cik\"].astype(str).str.zfill(10)\n","    return df[[\"Ticker\", \"CIK\"]]\n","\n","def load_reitnotes() -> pd.DataFrame:\n","    \"\"\"\n","    Scrapes REIT types from REITNotes.com.\n","\n","    Returns:\n","        DataFrame with 'Ticker' and simplified 'NotesType' (Equity or Mortgage).\n","    \"\"\"\n","    url = \"https://www.reitnotes.com/reit-list/\"\n","    html = requests.get(url, headers=UA, timeout=30).text\n","    df = pd.read_html(html)[0][[\"Symbol\", \"Type\"]].rename(columns={\"Symbol\": \"Ticker\", \"Type\": \"NotesType\"})\n","    df[\"Ticker\"] = df[\"Ticker\"].str.upper()\n","    df[\"NotesType\"] = df[\"NotesType\"].str.replace(\" REIT\", \"\", regex=False)\n","    df[\"NotesType\"] = df[\"NotesType\"].map({\"Equity\": \"Equity\", \"Mortgage\": \"Mortgage\"})\n","    return df\n","\n","def load_nareit(max_pages=25) -> pd.DataFrame:\n","    \"\"\"\n","    Scrapes REIT investment styles from Nareit.org directory pages.\n","\n","    Args:\n","        max_pages: Maximum number of paginated results to fetch.\n","\n","    Returns:\n","        DataFrame with 'Ticker' and 'NareitStyle' (Equity or Mortgage).\n","    \"\"\"\n","    rows = []\n","    for page in range(max_pages):\n","        url = f\"https://www.reit.com/investing/reit-directory?page={page}\"\n","        html = requests.get(url, headers=UA, timeout=30).text\n","        soup = BeautifulSoup(html, \"lxml\")\n","        table = soup.find(\"table\")\n","        if not table or not table.tbody:\n","            break  # No more data\n","        for tr in table.tbody.find_all(\"tr\"):\n","            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n","            if len(tds) < 3:\n","                continue\n","            ticker_raw = tds[1].split(\"|\")[0].strip()\n","            style = \"Mortgage\" if tds[-1].lower() == \"mortgage\" else \"Equity\"\n","            rows.append({\"Ticker\": ticker_raw.upper(), \"NareitStyle\": style})\n","        time.sleep(0.1)  # Be kind to the server\n","    return pd.DataFrame(rows).drop_duplicates()\n","\n","def resolve_style(row) -> str | None:\n","    \"\"\"\n","    Resolves the final REIT investment style by reconciling values from REITNotes and Nareit.\n","\n","    Args:\n","        row: A single DataFrame row containing NotesType and NareitStyle.\n","\n","    Returns:\n","        Investment style: Equity, Mortgage, Hybrid, or None.\n","    \"\"\"\n","    a, b = row[\"NotesType\"], row[\"NareitStyle\"]\n","    if pd.isna(a) and pd.isna(b):\n","        return None\n","    if a == b:\n","        return a\n","    if pd.isna(a):\n","        return b\n","    if pd.isna(b):\n","        return a\n","    return \"Hybrid\"\n","\n","def classify_styles(tickers: List[str]) -> pd.DataFrame:\n","    \"\"\"\n","    Classifies investment styles for a list of REIT tickers using external data sources.\n","\n","    Args:\n","        tickers: List of ticker symbols.\n","\n","    Returns:\n","        DataFrame with 'Ticker' and resolved 'InvestmentStyle'.\n","    \"\"\"\n","    sec = load_sec_master()\n","    notes = load_reitnotes()\n","    nareit = load_nareit()\n","\n","    base = pd.DataFrame({\"Ticker\": [t.upper() for t in tickers]})\n","    merged = (\n","        base\n","        .merge(sec, on=\"Ticker\", how=\"left\")\n","        .merge(notes, on=\"Ticker\", how=\"left\")\n","        .merge(nareit, on=\"Ticker\", how=\"left\")\n","    )\n","    merged[\"InvestmentStyle\"] = merged.apply(resolve_style, axis=1)\n","    return merged[[\"Ticker\", \"InvestmentStyle\"]]\n","\n","\n","# --- Build PortfolioGeneralInformation Table ---\n","\n","# Define static portfolios with associated metadata\n","portfolios = {\n","    \"REITPORTFOLIO001\": {\n","        \"NAME\": \"Core U.S. Equity REITs\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO002\": {\n","        \"NAME\": \"Real Estate Services & Platforms\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO003\": {\n","        \"NAME\": \"Global Real Estate Diversified\",\n","        \"PRODUCTCODE\": \"REITProd002\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO004\": {\n","        \"NAME\": \"Mortgage & Hybrid REITs\",\n","        \"PRODUCTCODE\": \"REITProd002\",\n","        \"INVESTMENTSTYLE\": \"Hybrid\"\n","    },\n","    \"REITPORTFOLIO005\": {\n","        \"NAME\": \"Industrial, Retail & Infrastructure\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    }\n","}\n","\n","# Construct each row of PortfolioGeneralInformation\n","rows = []\n","for code, meta in portfolios.items():\n","    rows.append({\n","        \"PORTFOLIOCODE\": code,\n","        \"NAME\": meta[\"NAME\"],\n","        \"INVESTMENTSTYLE\": meta[\"INVESTMENTSTYLE\"],\n","        \"PORTFOLIOCATEGORY\": \"REIT\",\n","        \"OPENDATE\": pd.to_datetime(\"2020-01-01\"),\n","        \"PERFORMANCEINCEPTIONDATE\": pd.to_datetime(\"2020-01-01\"),\n","        \"TERMINATIONDATE\": pd.NaT,\n","        \"BASECURRENCYCODE\": \"USD\",\n","        \"BASECURRENCYNAME\": \"US Dollar\",\n","        \"ISBEGINOFDAYPERFORMANCE\": True,\n","        \"PRODUCTCODE\": meta[\"PRODUCTCODE\"]\n","    })\n","\n","# Convert to final PGI DataFrame\n","pgi_df = pd.DataFrame(rows)\n","print(pgi_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiP9Q__0r_mt","executionInfo":{"status":"ok","timestamp":1753997597167,"user_tz":240,"elapsed":1990,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"8683f018-203e-4902-a9ef-0d2793185037"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      PORTFOLIOCODE                                 NAME INVESTMENTSTYLE  \\\n","0  REITPORTFOLIO001               Core U.S. Equity REITs          Equity   \n","1  REITPORTFOLIO002     Real Estate Services & Platforms          Equity   \n","2  REITPORTFOLIO003       Global Real Estate Diversified          Equity   \n","3  REITPORTFOLIO004              Mortgage & Hybrid REITs          Hybrid   \n","4  REITPORTFOLIO005  Industrial, Retail & Infrastructure          Equity   \n","\n","  PORTFOLIOCATEGORY   OPENDATE PERFORMANCEINCEPTIONDATE TERMINATIONDATE  \\\n","0              REIT 2020-01-01               2020-01-01             NaT   \n","1              REIT 2020-01-01               2020-01-01             NaT   \n","2              REIT 2020-01-01               2020-01-01             NaT   \n","3              REIT 2020-01-01               2020-01-01             NaT   \n","4              REIT 2020-01-01               2020-01-01             NaT   \n","\n","  BASECURRENCYCODE BASECURRENCYNAME  ISBEGINOFDAYPERFORMANCE  PRODUCTCODE  \n","0              USD        US Dollar                     True  REITProd001  \n","1              USD        US Dollar                     True  REITProd001  \n","2              USD        US Dollar                     True  REITProd002  \n","3              USD        US Dollar                     True  REITProd002  \n","4              USD        US Dollar                     True  REITProd001  \n"]}]},{"cell_type":"code","source":["pgi_df.to_csv('pgi_df.csv', index=False)"],"metadata":{"id":"L5f_ueJ1WXQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Functional Code + Sample Validation, Error Handling, Incomplete Data + Edge Case Handling**"],"metadata":{"id":"McFU7dQb1F2c"}},{"cell_type":"markdown","source":["This code builds on the above functional code, but is broken out into separate scripts which can be put into .py files and run with Assette's larger Python scripts. Also, it has sample code for the above categories.\n","\n","- Steps 1 & 2 are breakdowns of the functional code from section \"Combining approaches...\" > \"Populating PGI Table and mapping portfolio codes {FINAL}\"\n","\n","- Steps 3 & 4 are the sample code for validation, errors, incompletions, and edge cases\n","\n","Note that these code blocks should be saved to .py files with their subheader name and structured into a folder with a module layout, within an ETL pipeline."],"metadata":{"id":"foGeJdND1ipg"}},{"cell_type":"markdown","source":["## 1) classify_styles.py"],"metadata":{"id":"vdf8Fyiu19F4"}},{"cell_type":"code","source":["\"\"\"\n","Purpose:\n","\n"," - Grabs the master list of all public U.S. companies from the SEC, including:\n","    - Ticker (e.g., \"O\", \"PLD\")\n","    - CIK (central index key, e.g., \"0000726728\")\n","\n","How it works:\n","\n","- Pulls from https://www.sec.gov/files/company_tickers_exchange.json\n","- Parses the JSON response into a pandas.DataFrame\n","- Standardizes the formatting (uppercase tickers, padded CIKs)\n","\n","Why it's important:\n","- Ensures pipeline works for any ticker — not just REITs\n","- CIKs are useful later to query full SEC filings or facts\n","\n","\"\"\"\n","\n","# classify_styles.py\n","import requests, pandas as pd, time\n","from bs4 import BeautifulSoup\n","from typing import List\n","\n","UA = {\"User-Agent\": \"Mozilla/5.0 (MSBA Capstone Bot/1.0)\"}\n","\n","def load_sec_master():\n","    url = \"https://www.sec.gov/files/company_tickers_exchange.json\"\n","    data = requests.get(url, headers=UA, timeout=30).json()\n","    fields = data[\"fields\"]\n","    df = pd.DataFrame(data[\"data\"], columns=fields)\n","    df[\"Ticker\"] = df[\"ticker\"].str.upper()\n","    df[\"CIK\"] = df[\"cik\"].astype(str).str.zfill(10)\n","    return df[[\"Ticker\", \"CIK\"]]\n","\n","def load_reitnotes():\n","    url = \"https://www.reitnotes.com/reit-list/\"\n","    html = requests.get(url, headers=UA, timeout=30).text\n","    df = pd.read_html(html)[0][[\"Symbol\", \"Type\"]]\n","    df.columns = [\"Ticker\", \"NotesType\"]\n","    df[\"Ticker\"] = df[\"Ticker\"].str.upper()\n","    df[\"NotesType\"] = df[\"NotesType\"].str.replace(\" REIT\", \"\", regex=False)\n","    return df[df[\"NotesType\"].isin([\"Equity\", \"Mortgage\"])]\n","\n","def load_nareit(max_pages=25):\n","    rows = []\n","    for page in range(max_pages):\n","        url = f\"https://www.reit.com/investing/reit-directory?page={page}\"\n","        html = requests.get(url, headers=UA, timeout=30).text\n","        soup = BeautifulSoup(html, \"lxml\")\n","        table = soup.find(\"table\")\n","        if not table or not table.tbody:\n","            break\n","        for tr in table.tbody.find_all(\"tr\"):\n","            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n","            if len(tds) < 3:\n","                continue\n","            ticker = tds[1].split(\"|\")[0].strip().upper()\n","            style = \"Mortgage\" if tds[-1].lower() == \"mortgage\" else \"Equity\"\n","            rows.append({\"Ticker\": ticker, \"NareitStyle\": style})\n","        time.sleep(0.1)\n","    return pd.DataFrame(rows).drop_duplicates()\n","\n","def resolve_style(row):\n","    a, b = row[\"NotesType\"], row[\"NareitStyle\"]\n","    if pd.isna(a) and pd.isna(b):\n","        return None\n","    if a == b:\n","        return a\n","    if pd.isna(a):\n","        return b\n","    if pd.isna(b):\n","        return a\n","    return \"Hybrid\"\n","\n","def classify_styles(tickers: List[str]) -> pd.DataFrame:\n","    sec = load_sec_master()\n","    notes = load_reitnotes()\n","    nareit = load_nareit()\n","\n","    base = pd.DataFrame({\"Ticker\": [t.upper() for t in tickers]})\n","    merged = (\n","        base.merge(sec, on=\"Ticker\", how=\"left\")\n","            .merge(notes, on=\"Ticker\", how=\"left\")\n","            .merge(nareit, on=\"Ticker\", how=\"left\")\n","    )\n","    merged[\"InvestmentStyle\"] = merged.apply(resolve_style, axis=1)\n","    return merged[[\"Ticker\", \"InvestmentStyle\"]]\n"],"metadata":{"id":"T9KeLHV11hr0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2) generate_pgi_df.py"],"metadata":{"id":"DOkFJ-SE2Qci"}},{"cell_type":"code","source":["\"\"\"\n","Purpose: Generates synthetic PortfolioGeneralInformation (PGI) data and organizing them under synthetic portfolios.\n","\n","How it works:\n","- Creates 5 Portfolios based on common ticker traits\n","- Maps the 5 portfolios to each of 2 synthetic products (REITProd001 and REITProd002)\n","- Constructs a PGI dataframe with: • Portfolio metadata (e.g. OPENDATE, CURRENCY, CATEGORY, etc.) • InvestmentStyle classification for each portfolio\n","\n","Why it’s important:\n","- Enables Assette to simulate realistic REIT portfolio data.\n","- Fully populates required columns for the PortfolioGeneralInformation table in Snowflake.\n","- Modularized for Colab-based prototyping and Snowflake integration.\n","\n","NOTE: This code block will not run in Colab because the local environment will not recognize \"classify_styles\", etc. as traditional packages. It will work in a Python pipeline where:\n","  - The .py files are stored together in a defined directory\n","  - The Python environment is executed as a package or script\n","  - All modules (reconcile_styles.py, validate.py, main.py, etc.) are treated as first-class imports\n","\n","\"\"\"\n","\n","# generate_pgi.py\n","import pandas as pd\n","from classify_styles import classify_styles\n","\n","# Define static portfolios\n","# Define static portfolios\n","portfolios = {\n","    \"REITPORTFOLIO001\": {\n","        \"NAME\": \"Core U.S. Equity REITs\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO002\": {\n","        \"NAME\": \"Real Estate Services & Platforms\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO003\": {\n","        \"NAME\": \"Global Real Estate Diversified\",\n","        \"PRODUCTCODE\": \"REITProd002\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    },\n","    \"REITPORTFOLIO004\": {\n","        \"NAME\": \"Mortgage & Hybrid REITs\",\n","        \"PRODUCTCODE\": \"REITProd002\",\n","        \"INVESTMENTSTYLE\": \"Hybrid\"\n","    },\n","    \"REITPORTFOLIO005\": {\n","        \"NAME\": \"Industrial, Retail & Infrastructure\",\n","        \"PRODUCTCODE\": \"REITProd001\",\n","        \"INVESTMENTSTYLE\": \"Equity\"\n","    }\n","}\n","\n","\n","# Build PGI table statically\n","rows = []\n","for code, meta in portfolios.items():\n","    rows.append({\n","        \"PORTFOLIOCODE\": code,\n","        \"NAME\": meta[\"NAME\"],\n","        \"INVESTMENTSTYLE\": meta[\"INVESTMENTSTYLE\"],\n","        \"PORTFOLIOCATEGORY\": \"REIT\",\n","        \"OPENDATE\": pd.to_datetime(\"2020-01-01\"),\n","        \"PERFORMANCEINCEPTIONDATE\": pd.to_datetime(\"2020-01-01\"),\n","        \"TERMINATIONDATE\": pd.NaT,\n","        \"BASECURRENCYCODE\": \"USD\",\n","        \"BASECURRENCYNAME\": \"US Dollar\",\n","        \"ISBEGINOFDAYPERFORMANCE\": True,\n","        \"PRODUCTCODE\": meta[\"PRODUCTCODE\"]\n","    })\n","\n","# Final DataFrame\n","pgi_df = pd.DataFrame(rows)\n"],"metadata":{"id":"STI47WRz2j2N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3) validate.py"],"metadata":{"id":"QXyftd5m2ub0"}},{"cell_type":"code","source":["\"\"\"\n","Purpose:\n","- Adds data quality checks and optional retry logic for network calls.\n","\n","What it does:\n","\n","- Validates that each InvestmentStyle value is part of an approved list (including REIT and non-REIT asset classes).\n","- Logs a warning if any styles are missing (None) and suggests further investigation\n","- Logs an error and raises an exception if any styles are invalid or unrecognized\n","- Raises a critical alert if more than 30% of tickers have missing InvestmentStyle values\n","- Writes all validation results to a structured log file (investmentstyle_validation.log) with timestamps, function names, and line numbers\n","- Handles portfolio-level validation (not just tickers)\n","- Refers to the correct column name INVESTMENTSTYLE (used in the PGI table)\n","- Expands the log to include the PORTFOLIOCODE for any invalid entries\n","- Provides clear, structured logging and critical failure detection\n","\n","Why it's important:\n","\n","- Keeps data clean and predictable\n","- Helps to make the pipeline more fault-tolerant during web scraping or API flakiness\n","- Future-looking & inclusive to integrate other asset classes that Assette may encounter in their environment\n","\n","\n","\"\"\"\n","\n","# validate.py\n","import pandas as pd\n","import logging\n","\n","# Valid styles for REITs and other asset classes\n","VALID_STYLES = {\n","    \"Equity REIT\", \"Mortgage REIT\", \"Hybrid REIT\",\n","    \"Private Equity\", \"Hedge Fund\", \"Mutual Fund\",\n","    \"Alternative Asset\", \"Multi-Asset\",\n","    \"Other\", None\n","}\n","\n","# Set up logging\n","logging.basicConfig(\n","    filename=\"style_classification.log\",\n","    filemode=\"a\",\n","    format=\"%(asctime)s | %(levelname)s | %(funcName)s | line %(lineno)d | %(message)s\",\n","    level=logging.INFO\n",")\n","\n","def assert_styles(df: pd.DataFrame) -> None:\n","    \"\"\"\n","    Validates the INVESTMENTSTYLE column in the PGI table.\n","    Logs missing or invalid values and raises an error if thresholds are exceeded.\n","    \"\"\"\n","\n","    total = len(df)\n","    missing = df[\"INVESTMENTSTYLE\"].isna().sum()\n","    invalid = df[~df[\"INVESTMENTSTYLE\"].isin(VALID_STYLES)]\n","\n","    # Logging summary\n","    logging.info(f\"🧪 Validating {total} portfolios in INVESTMENTSTYLE...\")\n","\n","    if missing > 0:\n","        logging.warning(f\"{missing} portfolios have missing INVESTMENTSTYLE values.\")\n","\n","    if not invalid.empty:\n","        logging.error(f\"{len(invalid)} portfolios have invalid INVESTMENTSTYLE values:\")\n","        logging.error(f\"\\n{invalid[['PORTFOLIOCODE', 'INVESTMENTSTYLE']].to_string(index=False)}\")\n","\n","    # Raise error if more than 30% of styles are missing\n","    if missing / total > 0.3:\n","        raise ValueError(f\"Validation failed: Too many missing styles ({missing} / {total})\")\n","\n","    # Raise error on invalid styles\n","    if not invalid.empty:\n","        raise ValueError(\"Validation failed: Invalid INVESTMENTSTYLE values found.\")\n","\n","    if missing == 0 and invalid.empty:\n","        logging.info(\"INVESTMENTSTYLE validation passed with no issues.\")\n","    elif missing > 0:\n","        logging.info(\"ℹSuggest reviewing undefined styles — may need external classification.\")\n"],"metadata":{"id":"QK6-cW1-2yLm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4) main.py"],"metadata":{"id":"TC3T8LSF3Phj"}},{"cell_type":"code","source":["# main.py\n","\"\"\"\n","Main entry point for generating and uploading PGI data for REIT portfolios.\n","\n","Purpose:\n","Entry point that runs the full pipeline and optionally loads the result into Snowflake.\n","\n","What it does:\n","Calls all extractors and reconciliation functions\n","Validates the final DataFrame\n","Joins with a mock PortfolioGeneralInformation table\n","Prints the result (and optionally writes it to Snowflake using write_pandas())\n","\n","Why it's important:\n","\n","Ties all steps together\n","Can be scheduled or called from a larger system like Assette\n","\n","NOTE: This code block will not run in Colab because the local environment will not recognize \"classify_styles\", etc. as traditional packages.\n","It will work in a Python pipeline where:\n","- The .py files are stored together in a defined directory\n","- The Python environment is executed as a package or script\n","All modules (reconcile_styles.py, validate.py, main.py, etc.) are treated as first-class imports\n","\n","\"\"\"\n","\n","import pandas as pd\n","from classify_style import classify_styles\n","from generate_pgi_df import generate_pgi_df\n","from validate import assert_styles\n","from snowflake_loader import load_to_snowflake\n","\n","# Step 1: Classify investment styles\n","style_df = classify_styles(reit_tickers)\n","\n","# Step 2: Generate static PGI records\n","pgi_df = generate_pgi_df()\n","\n","# Step 3: Validate style assignments\n","assert_styles(style_df)\n","\n","# Step 4: Upload to Snowflake\n","load_to_snowflake(pgi_df)\n","\n"],"metadata":{"id":"rDqywXFM3QYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Connecting to Snowflake & Loading in Data**"],"metadata":{"id":"AztM9Zg6ktLu"}},{"cell_type":"markdown","source":["## Establishing Connection"],"metadata":{"id":"Z-reuIsGln39"}},{"cell_type":"markdown","source":["You must download the env.txt file which is located in my GDrive folder (\"Shiv-Code\") and upload using the \"Choose Files\" widget after running the below cell."],"metadata":{"id":"aCaqWuKnfCMD"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"oOwM03pkkw8y","executionInfo":{"status":"ok","timestamp":1753997644074,"user_tz":240,"elapsed":5250,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"27a3e0bc-6b75-4842-e312-504633f4bf9a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3b3a929b-5017-44a0-a9e5-d04002e6824f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3b3a929b-5017-44a0-a9e5-d04002e6824f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving env.txt to env.txt\n"]}]},{"cell_type":"code","source":["import os\n","\n","if os.path.exists(\"env.txt\"):\n","    os.rename(\"env.txt\", \"env\")\n","    print(\"Renamed env.txt to env\")\n","else:\n","    print(\"File not found. Make sure you uploaded env.txt.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOoYQafKk7vQ","executionInfo":{"status":"ok","timestamp":1753997645365,"user_tz":240,"elapsed":11,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"e4d74320-ecc1-4669-8b29-e8695356c86d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Renamed env.txt to env\n"]}]},{"cell_type":"code","source":["!pip install snowflake-connector-python python-dotenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPbMHknlk_L7","executionInfo":{"status":"ok","timestamp":1753997662609,"user_tz":240,"elapsed":16487,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"53c3c3a3-af38-4707-bce0-1f45300c5c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting snowflake-connector-python\n","  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n","  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting boto3>=1.24 (from snowflake-connector-python)\n","  Downloading boto3-1.40.0-py3-none-any.whl.metadata (6.7 kB)\n","Collecting botocore>=1.24 (from snowflake-connector-python)\n","  Downloading botocore-1.40.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n","Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n","Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n","Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (25.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.7.14)\n","Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n","Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n","Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n","Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n","Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python)\n","  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n","Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n","Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.40.0-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.40.0-py3-none-any.whl (13.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: asn1crypto, python-dotenv, jmespath, botocore, s3transfer, boto3, snowflake-connector-python\n","Successfully installed asn1crypto-1.5.1 boto3-1.40.0 botocore-1.40.0 jmespath-1.0.1 python-dotenv-1.1.1 s3transfer-0.13.1 snowflake-connector-python-3.16.0\n"]}]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(\"env\")  # This loads the Snowflake credentials into the environment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-e_ija5ylBIe","executionInfo":{"status":"ok","timestamp":1753997750649,"user_tz":240,"elapsed":8,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"5d5546da-7a99-43cb-b620-66944be86232"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import snowflake.connector\n","import os\n","\n","# Pull in credentials\n","conn = snowflake.connector.connect(\n","    user=os.getenv(\"SNOWFLAKE_USER\"),\n","    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n","    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n","    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n","    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n","    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n","    schema=os.getenv(\"SNOWFLAKE_SCHEMA\")\n",")\n","\n","# Run a simple test query\n","cur = conn.cursor()\n","cur.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE();\")\n","for row in cur:\n","    print(row)\n","\n","cur.close()\n","conn.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80emU-fllD84","executionInfo":{"status":"ok","timestamp":1753997753166,"user_tz":240,"elapsed":1586,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"outputId":"1b406d1b-d1e1-46f6-cf16-5d86623bf5ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('SHIVNEETN', 'AST_REALESTATE_DB_RW', 'AST_REALESTATE_DB')\n"]}]},{"cell_type":"markdown","source":["## Updating'InvestmentStyle' column with real-time data (from functional code)"],"metadata":{"id":"vrBWriaN_cVd"}},{"cell_type":"code","source":["from snowflake.connector import connect\n","from snowflake.connector.pandas_tools import write_pandas\n","import os\n","\n","# -----------------------------\n","# This script uploads the PortfolioGeneralInformation (PGI) DataFrame to Snowflake.\n","# It first ensures date columns are properly formatted,\n","# then loads the data into a staging table, and finally merges it into the target PGI table.\n","# -----------------------------\n","\n","# Ensure date columns are in proper datetime.date format (required for Snowflake DATE fields)\n","pgi_df[\"OPENDATE\"] = pd.to_datetime(pgi_df[\"OPENDATE\"])\n","pgi_df[\"PERFORMANCEINCEPTIONDATE\"] = pd.to_datetime(pgi_df[\"PERFORMANCEINCEPTIONDATE\"])\n","pgi_df[\"TERMINATIONDATE\"] = pd.to_datetime(pgi_df[\"TERMINATIONDATE\"])\n","\n","# Strip time component to make sure dates load into Snowflake DATE columns (not TIMESTAMP)\n","for col in [\"OPENDATE\", \"PERFORMANCEINCEPTIONDATE\", \"TERMINATIONDATE\"]:\n","    pgi_df[col] = pd.to_datetime(pgi_df[col]).dt.date\n","\n","# -----------------------------\n","# Connect to Snowflake using environment credentials\n","# -----------------------------\n","conn = connect(\n","    user=os.getenv(\"SNOWFLAKE_USER\"),\n","    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n","    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n","    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n","    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n","    database=\"AST_REALESTATE_DB\",\n","    schema=\"DBO\"\n",")\n","\n","# -----------------------------\n","# Create a staging table to temporarily hold PGI data\n","# -----------------------------\n","with conn.cursor() as cur:\n","    cur.execute(\"USE DATABASE AST_REALESTATE_DB\")\n","    cur.execute(\"USE SCHEMA DBO\")\n","\n","    cur.execute(\"\"\"\n","        CREATE OR REPLACE TEMP TABLE STG_PGI (\n","            PORTFOLIOCODE VARCHAR,\n","            NAME VARCHAR,\n","            INVESTMENTSTYLE VARCHAR,\n","            PORTFOLIOCATEGORY VARCHAR,\n","            OPENDATE DATE,\n","            PERFORMANCEINCEPTIONDATE DATE,\n","            BASECURRENCYCODE VARCHAR,\n","            BASECURRENCYNAME VARCHAR,\n","            ISBEGINOFDAYPERFORMANCE BOOLEAN,\n","            PRODUCTCODE VARCHAR,\n","            TERMINATIONDATE DATE\n","        )\n","    \"\"\")\n","\n","# -----------------------------\n","# Upload DataFrame to Snowflake staging table\n","# -----------------------------\n","success, nchunks, nrows, _ = write_pandas(\n","    conn, df=pgi_df, table_name=\"STG_PGI\", quote_identifiers=False\n",")\n","print(f\"Uploaded {nrows} rows to staging.\")\n","\n","# -----------------------------\n","# Merge staging data into the main PortfolioGeneralInformation table\n","# Rows are matched by PORTFOLIOCODE\n","# Existing rows are updated; new rows are inserted\n","# -----------------------------\n","with conn.cursor() as cur:\n","    cur.execute(\"\"\"\n","        MERGE INTO PortfolioGeneralInformation AS pgi\n","        USING STG_PGI AS stg\n","        ON pgi.PORTFOLIOCODE = stg.PORTFOLIOCODE\n","\n","        WHEN MATCHED THEN UPDATE SET\n","            pgi.NAME = stg.NAME,\n","            pgi.INVESTMENTSTYLE = stg.INVESTMENTSTYLE,\n","            pgi.PORTFOLIOCATEGORY = stg.PORTFOLIOCATEGORY,\n","            pgi.OPENDATE = stg.OPENDATE,\n","            pgi.PERFORMANCEINCEPTIONDATE = stg.PERFORMANCEINCEPTIONDATE,\n","            pgi.BASECURRENCYCODE = stg.BASECURRENCYCODE,\n","            pgi.BASECURRENCYNAME = stg.BASECURRENCYNAME,\n","            pgi.ISBEGINOFDAYPERFORMANCE = stg.ISBEGINOFDAYPERFORMANCE,\n","            pgi.PRODUCTCODE = stg.PRODUCTCODE,\n","            pgi.TERMINATIONDATE = stg.TERMINATIONDATE\n","\n","        WHEN NOT MATCHED THEN INSERT (\n","            PORTFOLIOCODE, NAME, INVESTMENTSTYLE, PORTFOLIOCATEGORY, OPENDATE,\n","            PERFORMANCEINCEPTIONDATE, BASECURRENCYCODE, BASECURRENCYNAME,\n","            ISBEGINOFDAYPERFORMANCE, PRODUCTCODE, TERMINATIONDATE\n","        ) VALUES (\n","            stg.PORTFOLIOCODE, stg.NAME, stg.INVESTMENTSTYLE, stg.PORTFOLIOCATEGORY,\n","            stg.OPENDATE, stg.PERFORMANCEINCEPTIONDATE, stg.BASECURRENCYCODE,\n","            stg.BASECURRENCYNAME, stg.ISBEGINOFDAYPERFORMANCE, stg.PRODUCTCODE,\n","            stg.TERMINATIONDATE\n","        )\n","    \"\"\")\n","    print(\"MERGE completed.\")"],"metadata":{"id":"aUYAY3O7j21j","executionInfo":{"status":"ok","timestamp":1753997786810,"user_tz":240,"elapsed":4610,"user":{"displayName":"Shivneet Nag","userId":"14807483772028410046"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c48b9e05-2c0e-4200-d78d-9a87edc83c8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Uploaded 5 rows to staging.\n","MERGE completed.\n"]}]},{"cell_type":"markdown","source":["# **Data Flow & Architecture Documentation (WIP)**"],"metadata":{"id":"PlHOb50oLu69"}},{"cell_type":"markdown","source":["## Source to Storage Mapping"],"metadata":{"id":"KvcCpTfCL0rQ"}},{"cell_type":"markdown","source":["| **Stage**                | **Description**                                                                                                                                 |\n","| ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n","| **Source**               | Ticker-based REIT classification sources:                                                                                                       |\n","|                          |   • [REITNotes](https://www.reitnotes.com/reit-list/)                                                                                           |\n","|                          |   • [Nareit REIT Directory](https://www.reit.com/investing/reit-directory)                                                                      |\n","|                          |   • [SEC Master Ticker List](https://www.sec.gov/files/company_tickers_exchange.json) (CIK, exchange, ticker metadata)                          |\n","| **Extraction**           | `classify_style.py` loads and parses the above sources using `requests`, `BeautifulSoup`, and `pandas.read_html()`                              |\n","| **Reconciliation**       | Styles are resolved using hybrid logic in `resolve_style()`:                                                                                    |\n","|                          |   • If both REITNotes and Nareit agree → use that style                                                                                         |\n","|                          |   • If both disagree → assign `\"Hybrid\"`                                                                                                        |\n","|                          |   • If only one is present → assign its style                                                                                                   |\n","|                          |   • If neither found → assign `None`                                                                                                            |\n","| **Portfolio Generation** | `generate_pgi.py` creates 5 synthetic portfolios  → 2-3 portfolios per product across 2 products (REITProd001 to REITProd002)        |\n","| **Validation**           | `validate.py` asserts:                                                                                                                          |\n","|                          |   • All styles are in an approved whitelist                                                                                                     |\n","|                          |   • No more than 30% are missing (`None`)                                                                                                       |\n","|                          |   • Logs bad rows with `Ticker`, `InvestmentStyle`, and context to `style_classification.log`                                                   |\n","| **Enrichment**           | `generate_pgi_df()` attaches metadata: Portfolio name, open date, strategy, currency, etc.                                                      |\n","| **Storage**              | Final DataFrame (`pgi_df`) is uploaded to `STG_PGI` (a temp staging table in Snowflake) using `write_pandas()`                                  |\n","|                          | A `MERGE` SQL statement inserts new `PORTFOLIOCODE`s and updates existing rows in the `PortfolioGeneralInformation` table                       |\n","| **Consumption**          | Assette’s fact sheet engine queries `PortfolioGeneralInformation.INVESTMENTSTYLE` for classification, disclosure, filtering, and business logic |\n"],"metadata":{"id":"nxk_1ArrL55O"}},{"cell_type":"markdown","source":["## Data Flow Diagram"],"metadata":{"id":"xoNcGhyaMPk_"}},{"cell_type":"markdown","source":["         +----------------------------+\n","         | SEC Master Ticker JSON     |\n","         |  ↳ https://sec.gov/...     |\n","         +----------------------------+\n","                     |\n","         +----------------------------+\n","         | REITNotes (read_html)      |\n","         |  ↳ https://reitnotes.com   |\n","         +----------------------------+\n","                     |\n","         +----------------------------+\n","         | Nareit Directory (scrape)  |\n","         |  ↳ https://reit.com/...    |\n","         +----------------------------+\n","                     |\n","                     v\n","         +-----------------------------+\n","         | classify_style.py           |   <-- classify_styles(ticker_list)\n","         +-----------------------------+\n","                     |\n","         +-----------------------------+\n","         | validate.py                 |   <-- assert_styles(df)\n","         |  ↳ logs to style_classification.log\n","         +-----------------------------+\n","                     |\n","         +-----------------------------+\n","         | generate_pgi.py             |   <-- generate_pgi_df(style_df)\n","         +-----------------------------+\n","                     |\n","         +-----------------------------+\n","         | main.py                     |   <-- orchestrates full pipeline\n","         |  ↳ write_pandas() to STG_PGI\n","         |  ↳ MERGE into PGI table\n","         +-----------------------------+\n","                     |\n","                     v\n","         +----------------------------------------------+\n","         | Snowflake Table: PortfolioGeneralInformation |\n","         |  ↳ INVESTMENTSTYLE + metadata inserted       |\n","         +----------------------------------------------+\n"],"metadata":{"id":"cfOri5YkMUPu"}},{"cell_type":"markdown","source":["## Transformation Logic Documentation"],"metadata":{"id":"ykzksvh0MZo6"}},{"cell_type":"markdown","source":["| **Transformation**              | **Description**                                                                                                   |\n","| ------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n","| **Web scraping**                | REITNotes and Nareit data extracted using BeautifulSoup, requests, and pandas                                     |\n","| **Hybrid Resolution Logic**     | If both sources agree → use it<br>If both disagree → `\"Hybrid\"`<br>If one available → use it<br>If neither → None |\n","| **Portfolio Allocation**        | 5 portfolios mapped under 2 products            |\n","| **Metadata Enrichment**         | Adds: `NAME`, `OPENDATE`, `STRATEGY`, `BASECURRENCY`, `PRODUCTCODE`, etc. using hardcoded/synthetic defaults      |\n","| **Validation (assert\\_styles)** | Ensures styles conform to internal REIT/asset type whitelist                                                      |\n","| **Data Types**                  | Styles and metadata stored as `VARCHAR`, `BOOLEAN`, and `DATE` in Snowflake                                       |\n","| **Insert Logic**                | MERGE SQL performs insert-or-update behavior on `PortfolioGeneralInformation`                                     |\n","| **Logging**                     | All validation errors recorded in `style_classification.log` with timestamps and line-level diagnostics           |\n"],"metadata":{"id":"Gj1UpU78Mcnd"}},{"cell_type":"markdown","source":["## Technical Notes"],"metadata":{"id":"3W5DNN9yMie6"}},{"cell_type":"markdown","source":["- Modular pipeline split across:\n","\n","  - classify_style.py → scraping + REIT classification\n","\n","  - validate.py → validator for InvestmentStyle column\n","\n","  - generate_pgi.py → 5-portfolio generation + enrichment\n","\n","  - main.py → orchestrator\n","\n","  - snowflake_loader.py → staging + merge to Snowflake\n","\n","- All portfolio + product codes are synthetic (e.g. PORTFOLIOCODE = REITPORTFOLIO001 → REITProd001)\n","\n","- Final output stored in Snowflake table: PortfolioGeneralInformation\n","\n","- Merge SQL uses PORTFOLIOCODE as the primary key\n","\n","- Designed to support both initial inserts and subsequent updates"],"metadata":{"id":"4iDiKcp6MlXZ"}}]}