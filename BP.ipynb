{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QPFoDbFg2Dq3",
        "Z-reuIsGln39",
        "R_Lx6qo_-46X",
        "gPq_PSF8DFwi",
        "Y6_dg_bAD_RE",
        "oYZ_UkjfRxTL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- **What it is:**  \n",
        "  - A script to fetch historical performance data for REIT benchmarks from Yahoo Finance, including their inception dates.\n",
        "\n",
        "- **What it does:**  \n",
        "  - Connects to Snowflake to get benchmark symbols.  \n",
        "  - Downloads and normalizes historical price data for each benchmark.  \n",
        "  - Retrieves or estimates each benchmark’s inception date.  \n",
        "  - Prepares a cleaned dataset for further analysis or upload.\n",
        "\n",
        "- **Why it’s important:**  \n",
        "  - Provides accurate and complete benchmark data for portfolio performance evaluation.  \n",
        "  - Ensures benchmark history includes inception dates for context.  \n",
        "  - Supports reliable reporting and analysis in real estate investment management.\n"
      ],
      "metadata": {
        "id": "luiuwY7pii0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Benchmark Performance Table**"
      ],
      "metadata": {
        "id": "QPFoDbFg2Dq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Establishing Snowflake Connection"
      ],
      "metadata": {
        "id": "Z-reuIsGln39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must download the env.txt file which is located in my GDrive folder (\"Shiv-Code\") and upload using the \"Choose Files\" widget after running the below cell."
      ],
      "metadata": {
        "id": "aCaqWuKnfCMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "oOwM03pkkw8y",
        "outputId": "70bdd3e7-251b-4634-8347-ba2e16f1f424"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f91949a3-85c7-4513-b848-0b55b4243052\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f91949a3-85c7-4513-b848-0b55b4243052\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving env.txt to env.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(\"env.txt\"):\n",
        "    os.rename(\"env.txt\", \"env\")\n",
        "    print(\"Renamed env.txt to env\")\n",
        "else:\n",
        "    print(\"File not found. Make sure you uploaded env.txt.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOoYQafKk7vQ",
        "outputId": "d39d816b-32a8-42fb-9d5b-95e1c796b2a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed env.txt to env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-connector-python python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPbMHknlk_L7",
        "outputId": "fe538386-46d5-46b0-fa62-1e6995eecc68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting boto3>=1.24 (from snowflake-connector-python)\n",
            "  Downloading boto3-1.40.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore>=1.24 (from snowflake-connector-python)\n",
            "  Downloading botocore-1.40.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (0.13.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.17.0)\n",
            "Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.1-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, python-dotenv, jmespath, botocore, s3transfer, boto3, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 boto3-1.40.1 botocore-1.40.1 jmespath-1.0.1 python-dotenv-1.1.1 s3transfer-0.13.1 snowflake-connector-python-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"env\")  # This loads the Snowflake credentials into the environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e_ija5ylBIe",
        "outputId": "aeeb622b-7054-4544-d69e-72c17fba13eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snowflake.connector\n",
        "import os\n",
        "\n",
        "# Pull in credentials\n",
        "conn = snowflake.connector.connect(\n",
        "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "    role=os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "    schema=os.getenv(\"SNOWFLAKE_SCHEMA\")\n",
        ")\n",
        "\n",
        "# Run a simple test query\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_DATABASE();\")\n",
        "for row in cur:\n",
        "    print(row)\n",
        "\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80emU-fllD84",
        "outputId": "136da0bf-3e50-4813-d14f-4a4fb89b6357"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('SHIVNEETN', 'AST_REALESTATE_DB_RW', 'AST_REALESTATE_DB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to BGI to pull Benchmark Codes"
      ],
      "metadata": {
        "id": "R_Lx6qo_-46X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfpiItcJ3QST",
        "outputId": "1a830699-ec7a-49d8-d0a6-22eb98848d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from snowflake.connector import connect\n",
        "import os\n",
        "\n",
        "def fetch_benchmark_performance_with_inception():\n",
        "    \"\"\"\n",
        "    Fetches benchmark performance data for REIT benchmarks from Yahoo Finance, including inception dates,\n",
        "    then prepares the data for upload.\n",
        "\n",
        "    Steps:\n",
        "    1. Connects to Snowflake and retrieves REIT benchmark codes and symbols from BenchmarkGeneralInformation.\n",
        "    2. Defines the historical data range from 2022-01-01 to today.\n",
        "    3. For each benchmark symbol:\n",
        "       - Cleans the symbol for Yahoo Finance usage.\n",
        "       - Attempts to retrieve the inception date from ticker info keys such as 'startDate' or 'firstTradeDate'.\n",
        "         If unavailable, uses the earliest available historical price date as a fallback.\n",
        "       - Downloads historical price data within the date range.\n",
        "       - Normalizes the price series to start at 100 for relative performance.\n",
        "       - Populates static columns such as BENCHMARKCODE, currency info, and performance frequency.\n",
        "       - Sets the HISTORYDATE column to the inception date for all rows.\n",
        "       - Adds HISTORYDATE1 as the date component of the price date.\n",
        "    4. Concatenates all individual benchmark data into one DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Concatenated DataFrame of benchmark performance data ready for upload.\n",
        "    \"\"\"\n",
        "    # Connect to Snowflake\n",
        "    conn = connect(\n",
        "        user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "        password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "        account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "        role=os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "        warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "        database=\"AST_REALESTATE_DB\",\n",
        "        schema=\"DBO\"\n",
        "    )\n",
        "\n",
        "    # Query to get REIT benchmarks with valid symbols\n",
        "    bgi_df = pd.read_sql(\"\"\"\n",
        "        SELECT BENCHMARKCODE, SYMBOL\n",
        "        FROM BenchmarkGeneralInformation\n",
        "        WHERE BENCHMARKCODE LIKE 'REITBENCH%'\n",
        "          AND SYMBOL IS NOT NULL\n",
        "    \"\"\", conn)\n",
        "\n",
        "    # Define date range for historical data\n",
        "    start_date = \"2022-01-01\"\n",
        "    end_date = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for _, row in bgi_df.iterrows():\n",
        "        symbol_raw = row[\"SYMBOL\"]\n",
        "        symbol = symbol_raw.replace(\"$\", \"\").strip()\n",
        "        code = row[\"BENCHMARKCODE\"]\n",
        "\n",
        "        try:\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            info = ticker.info\n",
        "\n",
        "            # Attempt to extract inception date from info\n",
        "            inception_date = None\n",
        "            if \"startDate\" in info and info[\"startDate\"]:\n",
        "                inception_date = pd.to_datetime(info[\"startDate\"], unit='s')\n",
        "            elif \"firstTradeDate\" in info and info[\"firstTradeDate\"]:\n",
        "                inception_date = pd.to_datetime(info[\"firstTradeDate\"], unit='s')\n",
        "            else:\n",
        "                # Fallback to earliest price date if inception date is unavailable\n",
        "                hist = ticker.history(start=start_date, end=end_date)\n",
        "                if hist.empty:\n",
        "                    print(f\"No data for {symbol}\")\n",
        "                    continue\n",
        "                inception_date = hist.index.min()\n",
        "\n",
        "            # Fetch historical price data\n",
        "            hist = ticker.history(start=start_date, end=end_date)\n",
        "            if hist.empty:\n",
        "                print(f\"No historical data for {symbol}\")\n",
        "                continue\n",
        "            hist = hist.reset_index()\n",
        "\n",
        "            # Normalize values to 100 at start\n",
        "            if \"Adj Close\" in hist.columns:\n",
        "                base_price = hist[\"Adj Close\"].iloc[0]\n",
        "                hist[\"VALUE\"] = (hist[\"Adj Close\"] / base_price) * 100\n",
        "            elif \"Close\" in hist.columns:\n",
        "                base_price = hist[\"Close\"].iloc[0]\n",
        "                hist[\"VALUE\"] = (hist[\"Close\"] / base_price) * 100\n",
        "            else:\n",
        "                print(f\"No 'Adj Close' or 'Close' data for {symbol}\")\n",
        "                continue\n",
        "\n",
        "            # Populate static metadata columns\n",
        "            hist[\"BENCHMARKCODE\"] = code\n",
        "            hist[\"PERFORMANCEDATATYPE\"] = \"Total Return\"\n",
        "            hist[\"CURRENCY\"] = \"US Dollar\"\n",
        "            hist[\"CURRENCYCODE\"] = \"USD\"\n",
        "            hist[\"PERFORMANCEFREQUENCY\"] = \"Daily\"\n",
        "\n",
        "            # Set HISTORYDATE column to inception date for all rows\n",
        "            hist[\"HISTORYDATE\"] = inception_date\n",
        "\n",
        "            # Extract date only for HISTORYDATE1\n",
        "            hist[\"HISTORYDATE1\"] = hist[\"Date\"].dt.date\n",
        "\n",
        "            # Append relevant columns to rows list\n",
        "            rows.append(hist[[\n",
        "                \"BENCHMARKCODE\", \"PERFORMANCEDATATYPE\", \"CURRENCY\", \"CURRENCYCODE\",\n",
        "                \"PERFORMANCEFREQUENCY\", \"HISTORYDATE\", \"HISTORYDATE1\", \"VALUE\"\n",
        "            ]])\n",
        "\n",
        "            print(f\"Fetched data for {symbol} → {code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error for {symbol} → {e}\")\n",
        "\n",
        "    # Combine all benchmark data into one DataFrame or return empty if none\n",
        "    if rows:\n",
        "        bp_df = pd.concat(rows, ignore_index=True)\n",
        "    else:\n",
        "        print(\"No benchmark data returned.\")\n",
        "        bp_df = pd.DataFrame()\n",
        "\n",
        "    return bp_df\n",
        "\n",
        "# Usage:\n",
        "# bp_df = fetch_benchmark_performance_with_inception()\n",
        "# print(bp_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGH-FP88g4KW",
        "outputId": "60037bf8-0069-4311-c39a-648ff0022801"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1945075176.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
            "  bgi_df = pd.read_sql(\"\"\"\n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$BBREIT: possibly delisted; no timezone found\n",
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data for BBREIT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$FNREI: possibly delisted; no timezone found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data for FNREI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$RMZ: possibly delisted; no price data found  (1d 2022-01-01 -> 2025-08-01)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data for RMZ\n",
            "Fetched data for SCHH → REITBENCH03\n",
            "Fetched data for VNQ → REITBENCH01\n",
            "Fetched data for IYR → REITBENCH02\n",
            "     BENCHMARKCODE PERFORMANCEDATATYPE   CURRENCY CURRENCYCODE  \\\n",
            "0      REITBENCH03        Total Return  US Dollar          USD   \n",
            "1      REITBENCH03        Total Return  US Dollar          USD   \n",
            "2      REITBENCH03        Total Return  US Dollar          USD   \n",
            "3      REITBENCH03        Total Return  US Dollar          USD   \n",
            "4      REITBENCH03        Total Return  US Dollar          USD   \n",
            "...            ...                 ...        ...          ...   \n",
            "2686   REITBENCH02        Total Return  US Dollar          USD   \n",
            "2687   REITBENCH02        Total Return  US Dollar          USD   \n",
            "2688   REITBENCH02        Total Return  US Dollar          USD   \n",
            "2689   REITBENCH02        Total Return  US Dollar          USD   \n",
            "2690   REITBENCH02        Total Return  US Dollar          USD   \n",
            "\n",
            "     PERFORMANCEFREQUENCY               HISTORYDATE HISTORYDATE1       VALUE  \n",
            "0                   Daily 2022-01-03 00:00:00-05:00   2022-01-03  100.000000  \n",
            "1                   Daily 2022-01-03 00:00:00-05:00   2022-01-04   99.923562  \n",
            "2                   Daily 2022-01-03 00:00:00-05:00   2022-01-05   97.056580  \n",
            "3                   Daily 2022-01-03 00:00:00-05:00   2022-01-06   97.209481  \n",
            "4                   Daily 2022-01-03 00:00:00-05:00   2022-01-07   96.597878  \n",
            "...                   ...                       ...          ...         ...  \n",
            "2686                Daily 2022-01-03 00:00:00-05:00   2025-07-25   92.926032  \n",
            "2687                Daily 2022-01-03 00:00:00-05:00   2025-07-28   91.357298  \n",
            "2688                Daily 2022-01-03 00:00:00-05:00   2025-07-29   92.907020  \n",
            "2689                Daily 2022-01-03 00:00:00-05:00   2025-07-30   91.566463  \n",
            "2690                Daily 2022-01-03 00:00:00-05:00   2025-07-31   90.206894  \n",
            "\n",
            "[2691 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Code"
      ],
      "metadata": {
        "id": "gPq_PSF8DFwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "def validate_benchmark_performance(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validate the BENCHMARKPERFORMANCE table before inserting into Snowflake.\n",
        "\n",
        "    This validator checks for expected structure, nulls in key fields,\n",
        "    valid performance types, and flags extreme values for proxy NAV returns\n",
        "    and price data. It returns a summary DataFrame and logs all issues.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Performance-level REIT benchmark data with expected fields:\n",
        "        BENCHMARKCODE, PERFORMANCEDATATYPE, CURRENCYCODE, CURRENCY,\n",
        "        PERFORMANCEFREQUENCY, VALUE, HISTORYDATE.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        One-row summary with validation flags and issue counts.\n",
        "    \"\"\"\n",
        "\n",
        "    required_cols = [\n",
        "        \"BENCHMARKCODE\", \"PERFORMANCEDATATYPE\", \"CURRENCYCODE\", \"CURRENCY\",\n",
        "        \"PERFORMANCEFREQUENCY\", \"VALUE\", \"HISTORYDATE\"\n",
        "    ]\n",
        "\n",
        "    # Confirm required columns\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        logging.error(f\"[benchmark_performance] Missing required columns: {missing_cols}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Set of valid performance types (added \"Total Return\")\n",
        "    valid_types = {\"Proxy NAV Return\", \"Prices\", \"Total Return\"}\n",
        "\n",
        "    # Summary flags\n",
        "    summary = {\n",
        "        \"missing_benchmarkcodes\": df[\"BENCHMARKCODE\"].isnull().sum(),\n",
        "        \"missing_values\": df[\"VALUE\"].isnull().sum(),\n",
        "        \"invalid_performance_types\": (~df[\"PERFORMANCEDATATYPE\"].isin(valid_types)).sum(),\n",
        "        # Adjust outlier ranges as appropriate for Total Return data\n",
        "        \"nav_return_outliers\": df.loc[\n",
        "            df[\"PERFORMANCEDATATYPE\"] == \"Proxy NAV Return\"\n",
        "        ][\"VALUE\"].apply(lambda x: not (-1 <= x <= 2)).sum(),\n",
        "        \"total_return_outliers\": df.loc[\n",
        "            df[\"PERFORMANCEDATATYPE\"] == \"Total Return\"\n",
        "        ][\"VALUE\"].apply(lambda x: x <= 0).sum(),  # Total Return % usually positive, but you can adjust logic\n",
        "        \"price_outliers\": df.loc[\n",
        "            df[\"PERFORMANCEDATATYPE\"] == \"Prices\"\n",
        "        ][\"VALUE\"].apply(lambda x: x <= 0).sum()  # Prices shouldn't be <= 0\n",
        "    }\n",
        "\n",
        "    # Log issues\n",
        "    for key, val in summary.items():\n",
        "        if val:\n",
        "            logging.warning(f\"[benchmark_performance] {key.replace('_', ' ').capitalize()}: {val}\")\n",
        "\n",
        "    if not any(summary.values()):\n",
        "        logging.info(\"[benchmark_performance] Validation passed.\")\n",
        "\n",
        "    return pd.DataFrame([summary])\n"
      ],
      "metadata": {
        "id": "56RJ-MkJDFTD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Populating BP Table in Snowflake"
      ],
      "metadata": {
        "id": "Y6_dg_bAD_RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from snowflake.connector import connect\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def upload_and_merge_benchmark_performance(bp_df):\n",
        "    \"\"\"\n",
        "    Uploads benchmark performance data (bp_df) to Snowflake staging table and merges into the\n",
        "    target BENCHMARKPERFORMANCE table, handling duplicate rows by keeping only one record per key.\n",
        "\n",
        "    Args:\n",
        "        bp_df (pd.DataFrame): DataFrame containing benchmark performance data with columns:\n",
        "            BENCHMARKCODE, PERFORMANCEDATATYPE, CURRENCYCODE, CURRENCY, PERFORMANCEFREQUENCY,\n",
        "            VALUE, HISTORYDATE (timestamp), HISTORYDATE1 (date).\n",
        "\n",
        "    Steps:\n",
        "    1. Remove timezone info from HISTORYDATE for Snowflake compatibility.\n",
        "    2. Connect to Snowflake using environment variables for credentials.\n",
        "    3. Create or replace a temporary staging table with the appropriate schema.\n",
        "    4. Upload the bp_df data into the staging table using Snowflake's write_pandas utility.\n",
        "    5. Merge data from staging into BENCHMARKPERFORMANCE table with deduplication logic\n",
        "       using ROW_NUMBER() to avoid duplicate merge conflicts.\n",
        "    \"\"\"\n",
        "    # Step 1: Ensure HISTORYDATE is timezone-naive to match Snowflake TIMESTAMP_NTZ\n",
        "    bp_df[\"HISTORYDATE\"] = pd.to_datetime(bp_df[\"HISTORYDATE\"]).dt.tz_localize(None)\n",
        "\n",
        "    # Step 2: Establish connection to Snowflake\n",
        "    conn = connect(\n",
        "        user=os.getenv(\"SNOWFLAKE_USER\"),\n",
        "        password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
        "        account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "        role=os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "        warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "        database=\"AST_REALESTATE_DB\",\n",
        "        schema=\"DBO\"\n",
        "    )\n",
        "\n",
        "    with conn.cursor() as cur:\n",
        "        # Step 3: Set context and create temporary staging table\n",
        "        cur.execute(\"USE DATABASE AST_REALESTATE_DB\")\n",
        "        cur.execute(\"USE SCHEMA DBO\")\n",
        "\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE OR REPLACE TEMP TABLE STG_BENCHMARKPERFORMANCE (\n",
        "                BENCHMARKCODE VARCHAR,\n",
        "                PERFORMANCEDATATYPE VARCHAR,\n",
        "                CURRENCYCODE VARCHAR,\n",
        "                CURRENCY VARCHAR,\n",
        "                PERFORMANCEFREQUENCY VARCHAR,\n",
        "                VALUE FLOAT,\n",
        "                HISTORYDATE TIMESTAMP_NTZ,\n",
        "                HISTORYDATE1 DATE\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "    # Step 4: Upload DataFrame to staging table\n",
        "    success, nchunks, nrows, _ = write_pandas(\n",
        "        conn=conn,\n",
        "        df=bp_df,\n",
        "        table_name=\"STG_BENCHMARKPERFORMANCE\",\n",
        "        quote_identifiers=False,\n",
        "        use_logical_type=True  # Ensures correct data type handling\n",
        "    )\n",
        "    print(f\"Uploaded {nrows} rows to STG_BENCHMARKPERFORMANCE\")\n",
        "\n",
        "    # Step 5: Perform MERGE with deduplication using ROW_NUMBER()\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(\"\"\"\n",
        "            MERGE INTO BENCHMARKPERFORMANCE AS target\n",
        "            USING (\n",
        "                SELECT *\n",
        "                FROM (\n",
        "                    SELECT *,\n",
        "                        ROW_NUMBER() OVER (\n",
        "                            PARTITION BY BENCHMARKCODE, PERFORMANCEDATATYPE, CURRENCYCODE, PERFORMANCEFREQUENCY, HISTORYDATE\n",
        "                            ORDER BY VALUE DESC\n",
        "                        ) AS rn\n",
        "                    FROM STG_BENCHMARKPERFORMANCE\n",
        "                )\n",
        "                WHERE rn = 1\n",
        "            ) AS source\n",
        "            ON target.BENCHMARKCODE = source.BENCHMARKCODE\n",
        "               AND target.PERFORMANCEDATATYPE = source.PERFORMANCEDATATYPE\n",
        "               AND target.CURRENCYCODE = source.CURRENCYCODE\n",
        "               AND target.PERFORMANCEFREQUENCY = source.PERFORMANCEFREQUENCY\n",
        "               AND target.HISTORYDATE = source.HISTORYDATE\n",
        "\n",
        "            WHEN MATCHED THEN UPDATE SET\n",
        "                target.VALUE = source.VALUE,\n",
        "                target.CURRENCY = source.CURRENCY,\n",
        "                target.HISTORYDATE1 = source.HISTORYDATE1\n",
        "\n",
        "            WHEN NOT MATCHED THEN INSERT (\n",
        "                BENCHMARKCODE, PERFORMANCEDATATYPE, CURRENCYCODE, CURRENCY,\n",
        "                PERFORMANCEFREQUENCY, VALUE, HISTORYDATE, HISTORYDATE1\n",
        "            ) VALUES (\n",
        "                source.BENCHMARKCODE, source.PERFORMANCEDATATYPE, source.CURRENCYCODE, source.CURRENCY,\n",
        "                source.PERFORMANCEFREQUENCY, source.VALUE, source.HISTORYDATE, source.HISTORYDATE1\n",
        "            )\n",
        "        \"\"\")\n",
        "        print(\"MERGE into BENCHMARKPERFORMANCE completed.\")\n",
        "\n",
        "# Example usage:\n",
        "# upload_and_merge_benchmark_performance(bp_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0-xEaoShtHA",
        "outputId": "df9cb71d-a965-4e47-cfd4-494d143a5a77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded 2691 rows to STG_BENCHMARKPERFORMANCE\n",
            "MERGE into BENCHMARKPERFORMANCE completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Flow & Architecture Mapping**"
      ],
      "metadata": {
        "id": "oYZ_UkjfRxTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source to Storage Mapping"
      ],
      "metadata": {
        "id": "uzXh13MLR17n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Stage              | Description                                                                                               |\n",
        "| ------------------ | --------------------------------------------------------------------------------------------------------- |\n",
        "| **Source**         | Yahoo Finance historical data pulled dynamically using tickers from `BenchmarkGeneralInformation`         |\n",
        "| **Extraction**     | Python script uses `yfinance.Ticker.history()` to fetch adjusted close prices for benchmarks over time    |\n",
        "| **Transformation** | Normalize data to daily total return index starting at 100; add metadata columns like currency, frequency |\n",
        "|  • Normalize       | `VALUE = (Adj Close / first Adj Close) * 100` — relative total return index                               |\n",
        "|  • Add fields      | `PERFORMANCEDATATYPE=\"Total Return\"`, `CURRENCY=\"US Dollar\"`, `CURRENCYCODE=\"USD\"`, etc.                  |\n",
        "|  • Date fields     | Includes both `HISTORYDATE` (Timestamp\\_NTZ) and `HISTORYDATE1` (Date)                                    |\n",
        "| **Validation**     | `validate_benchmark_performance()` checks required columns, missing values, invalid types, outliers       |\n",
        "| **Storage**        | Upload via `write_pandas()` into staging table then MERGE into `BenchmarkPerformance`                     |\n",
        "| **Consumption**    | Used for performance reporting, benchmark tracking, portfolio analysis, UI visualizations                 |\n"
      ],
      "metadata": {
        "id": "1cgkvHBUR5vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Flow Diagram"
      ],
      "metadata": {
        "id": "cu5kgw-rR5nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "          +---------------------------+\n",
        "          | BenchmarkGeneralInformation |\n",
        "          | (Symbols from Snowflake)    |\n",
        "          +---------------------------+\n",
        "                       |\n",
        "               [Loop over each symbol]\n",
        "                       |\n",
        "                       v\n",
        "          +---------------------------+\n",
        "          | yfinance.Ticker.history() |\n",
        "          | (Adj Close daily prices)  |\n",
        "          +---------------------------+\n",
        "                       |\n",
        "                       v\n",
        "          +---------------------------+\n",
        "          | Normalize to index = 100  |\n",
        "          | Add metadata fields       |\n",
        "          +---------------------------+\n",
        "                       |\n",
        "                       v\n",
        "          +---------------------------+\n",
        "          | validate_benchmark_performance() |\n",
        "          +---------------------------+\n",
        "                       |\n",
        "                       v\n",
        "          +---------------------------+\n",
        "          | write_pandas() staging    |\n",
        "          | MERGE into BenchmarkPerformance |\n",
        "          +---------------------------+\n",
        "                       |\n",
        "                       v\n",
        "          +---------------------------+\n",
        "          | Reporting, UI, analytics  |\n",
        "          +---------------------------+\n"
      ],
      "metadata": {
        "id": "EZTUpbEeR-X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation Logic Documentation"
      ],
      "metadata": {
        "id": "HXYyr5sTSB1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value"
      ],
      "metadata": {
        "id": "xJJ_4H5wSGCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Transformation      | Description                                             |\n",
        "| ------------------- | ------------------------------------------------------- |\n",
        "| Index normalization | `VALUE = (Adj Close / first Adj Close) * 100`           |\n",
        "| Purpose             | Converts prices into total return index starting at 100 |\n",
        "| Data Type           | `FLOAT`                                                 |\n"
      ],
      "metadata": {
        "id": "G3dKSoPYSH_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PerformanceFrequency"
      ],
      "metadata": {
        "id": "0Fh4l9zBSJrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Transformation | Description                     |\n",
        "| -------------- | ------------------------------- |\n",
        "| Fixed          | `\"Daily\"`                       |\n",
        "| Purpose        | Frequency of price observations |\n",
        "| Data Type      | `VARCHAR`                       |\n"
      ],
      "metadata": {
        "id": "Tlviv6N9SMUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PerformanceDataType"
      ],
      "metadata": {
        "id": "7Tn6H0hRSPVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Transformation | Description                                  |\n",
        "| -------------- | -------------------------------------------- |\n",
        "| Fixed          | `\"Total Return\"`                             |\n",
        "| Purpose        | Indicates data represents total return index |\n",
        "| Data Type      | `VARCHAR`                                    |\n"
      ],
      "metadata": {
        "id": "uouPfNkPSRz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Date fields"
      ],
      "metadata": {
        "id": "OrPtXBX0STZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Field          | Description                                     | Data Type       |\n",
        "| -------------- | ----------------------------------------------- | --------------- |\n",
        "| `HISTORYDATE`  | Timestamp without timezone of each price record | `TIMESTAMP_NTZ` |\n",
        "| `HISTORYDATE1` | Date portion extracted from `HISTORYDATE`       | `DATE`          |\n"
      ],
      "metadata": {
        "id": "ljlmMbo7SVO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Currency / Currency Code"
      ],
      "metadata": {
        "id": "WsvT1tjRSZNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Transformation | Description                              |\n",
        "| -------------- | ---------------------------------------- |\n",
        "| Fixed          | `\"US Dollar\"` / `\"USD\"`                  |\n",
        "| Purpose        | Currency context of the benchmark prices |\n",
        "| Data Type      | `VARCHAR`                                |\n"
      ],
      "metadata": {
        "id": "JuCVZH95SjSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "lPfIqJXQSjKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Check                       | Description                                                                          |\n",
        "| --------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| Required columns            | `BENCHMARKCODE`, `VALUE`, `HISTORYDATE`, `PERFORMANCEDATATYPE`, etc. must be present |\n",
        "| Missing values              | Nulls in `VALUE` or key fields flagged                                               |\n",
        "| Performance type validation | Only accept `\"Total Return\"` or `\"Price\"`                                            |\n",
        "| Outlier detection           | Flags `VALUE` outside expected range (e.g., negative or extreme)                     |\n",
        "| Logging                     | Uses Python `logging` for warnings and errors                                        |\n"
      ],
      "metadata": {
        "id": "bj9I-GFcSoiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Enhancements"
      ],
      "metadata": {
        "id": "BgNXT8OvSuW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                     | Description                                                        |\n",
        "| --------------------------- | ------------------------------------------------------------------ |\n",
        "| Multi-frequency support     | Add weekly/monthly aggregations                                    |\n",
        "| Currency conversion         | Handle benchmarks priced in other currencies                       |\n",
        "| Historical adjustments      | Factor in dividends, splits, or rebalancing events                 |\n",
        "| Automated anomaly detection | More advanced outlier detection with machine learning              |\n",
        "| CI/CD integration           | Automate validation and ingestion as part of ETL pipelines         |\n",
        "| Incremental updates         | Support incremental data loads rather than full historical reloads |\n"
      ],
      "metadata": {
        "id": "IQZb-uGrSwhY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1I7JzCkdR0jA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}