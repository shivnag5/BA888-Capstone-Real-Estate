# -*- coding: utf-8 -*-
"""BenchmarkCharacteristics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9LtA2eknzGcvXGU3uIQlkPJKow7qllQ
"""

# Benchmark REIT Metrics Extraction and Upload to Snowflake
#
# This script connects to Snowflake, extracts REIT benchmark codes, pulls historical
# price data from Yahoo Finance, calculates performance metrics (monthly return,
# volatility, Sharpe ratio, etc.), and writes the results to BENCHMARKCHARACTERISTICS_NEW.

import os
import pandas as pd
import numpy as np
import yfinance as yf
from dotenv import load_dotenv
from snowflake.connector import connect
from snowflake.connector.pandas_tools import write_pandas

# Step 1: Load environment variables (credentials, config) from a .env file
load_dotenv(".env")

# Optional: Enable upload of .env.txt when running in Google Colab
try:
    from google.colab import files
    uploaded = files.upload()
    if ".env.txt" in uploaded:
        os.rename(".env.txt", ".env")
except ImportError:
    pass

# Step 2: Establish Snowflake connection using environment variables
conn = connect(
    user=os.getenv("SNOWFLAKE_USER"),
    password=os.getenv("SNOWFLAKE_PASSWORD"),
    account=os.getenv("SNOWFLAKE_ACCOUNT"),
    role=os.getenv("SNOWFLAKE_ROLE"),
    warehouse=os.getenv("SNOWFLAKE_WAREHOUSE"),
    database="AST_REALESTATE_DB",
    schema="DBO"
)

# Step 3: Query REIT benchmark tickers from Snowflake
query = "SELECT BENCHMARKCODE, SYMBOL FROM BenchmarkGeneralInformation"
df_bgi = pd.read_sql(query, conn)

# Only include REIT benchmarks of interest
reit_symbols = ["VNQ", "IYR", "SCHH"]
df_bgi = df_bgi[df_bgi["SYMBOL"].isin(reit_symbols)]

# Create a dictionary mapping SYMBOL to BENCHMARKCODE
symbol_to_code = dict(zip(df_bgi["SYMBOL"], df_bgi["BENCHMARKCODE"]))


def calculate_reit_metrics(ticker: str, benchmark_code: str) -> pd.DataFrame:
    """
    Download historical prices and calculate:
    - Monthly Return
    - Annualized Volatility (12-month)
    - Sharpe Ratio (12-month, excess over 2% annual risk-free rate)
    - 12M Rolling Return (cumulative)
    - Max Drawdown over 12 months

    Returns a DataFrame formatted to match Snowflake table schema.
    """
    # Pull full historical daily data for the REIT ticker
    full_daily = yf.download(ticker, period="max", interval="1d", auto_adjust=True)
    if full_daily.empty:
        raise ValueError(f"No data found for {ticker}")

    # Create a synthetic first-of-month price row ("stub") to fill gaps
    first_date = full_daily.index.min()
    stub_price = float(full_daily.loc[first_date, "Close"])
    stub_date = pd.Timestamp(f"{first_date.year}-{first_date.month:02d}-01")
    stub_row = pd.DataFrame({"Monthly_Close": [stub_price]}, index=[stub_date])

    # Pull monthly adjusted prices starting after the stub
    start_month = (stub_date + pd.offsets.MonthBegin(1)).strftime("%Y-%m-%d")
    monthly_df = yf.download(
        ticker,
        start=start_month,
        end="2024-12-31",
        interval="1mo",
        auto_adjust=True
    )

    # Flatten column index if necessary
    if isinstance(monthly_df.columns, pd.MultiIndex):
        monthly_df.columns = [col[0] for col in monthly_df.columns]

    # Rename and clean monthly closing prices
    monthly_df = monthly_df[["Close"]].rename(columns={"Close": "Monthly_Close"}).dropna()

    # Combine stub row and monthly prices
    combined_df = pd.concat([stub_row, monthly_df]).sort_index()

    # Calculate monthly return
    combined_df["Monthly_Return"] = combined_df["Monthly_Close"].pct_change()

    # Calculate rolling 12-month annualized volatility
    combined_df["Annualized_Volatility"] = (
        combined_df["Monthly_Return"].rolling(12).std() * np.sqrt(12)
    )

    # Calculate Sharpe Ratio using excess return over risk-free rate (2% annual)
    rf_monthly = 0.02 / 12
    excess = combined_df["Monthly_Return"] - rf_monthly
    mean_excess = excess.rolling(12).mean()
    std_excess = excess.rolling(12).std()
    combined_df["Sharpe_Ratio"] = np.where(
        std_excess > 0.01,
        (mean_excess / std_excess) * np.sqrt(12),
        np.nan
    )

    # Set NaN for Sharpe Ratio for the first 11 months
    combined_df.loc[combined_df.index[:11], "Sharpe_Ratio"] = np.nan

    # Calculate rolling 12-month total return
    combined_df["Rolling_12M_Return"] = (
        (1 + combined_df["Monthly_Return"]).rolling(12).apply(np.prod, raw=True) - 1
    )

    # Define function to calculate max drawdown over a rolling 12-month window
    def max_drawdown(series):
        cumulative = (1 + series).cumprod()
        peak = cumulative.cummax()
        return ((cumulative - peak) / peak).min()

    # Apply max drawdown calculation
    combined_df["Max_Drawdown_12M"] = (
        combined_df["Monthly_Return"].rolling(12).apply(max_drawdown)
    )

    # Align all rows to month-end and reset index
    combined_df.index += pd.offsets.MonthEnd(0)
    combined_df = combined_df.reset_index().rename(columns={"index": "HISTORYDATE"})

    # Map each calculated metric to a display label
    metric_labels = {
        "Monthly_Return": "Monthly Return",
        "Annualized_Volatility": "Annualized Volatility",
        "Sharpe_Ratio": "Sharpe Ratio",
        "Rolling_12M_Return": "12M Rolling Return",
        "Max_Drawdown_12M": "12M Max Drawdown"
    }

    # Format each metric into the Snowflake-compatible record structure
    records = []
    for _, row in combined_df.iterrows():
        for metric, label in metric_labels.items():
            records.append({
                "BENCHMARKCODE": benchmark_code,
                "CURRENCYCODE": "USD",
                "CURRENCY": "US Dollar",
                "LANGUAGECODE": "EN",
                "CATEGORY": "Performance",
                "CATEGORYNAME": "Monthly Performance Metrics",
                "CHARACTERISTICNAME": metric,
                "CHARACTERISTICDISPLAYNAME": label,
                "STATISTICTYPE": "Value",
                "CHARACTERISTICVALUE": f"{row[metric]:.6f}" if pd.notnull(row[metric]) else None,
                "ABBREVIATEDTEXT": "",
                "HISTORYDATE": row["HISTORYDATE"].date()
            })

    return pd.DataFrame(records)


# Step 5: Loop through all benchmark tickers and calculate metrics
all_rows = []
for symbol, benchmark_code in symbol_to_code.items():
    try:
        print(f"Processing {symbol}...")
        df = calculate_reit_metrics(symbol, benchmark_code)
        all_rows.append(df)
    except Exception as e:
        print(f"Skipping {symbol} due to error: {e}")

# Combine all results into a single DataFrame
final_df = pd.concat(all_rows, ignore_index=True)

# Step 6: Create the Snowflake destination table if it doesn't exist
create_table_sql = """
CREATE OR REPLACE TABLE AST_REALESTATE_DB.DBO.BENCHMARKCHARACTERISTICS_NEW (
    BENCHMARKCODE VARCHAR(50),
    CURRENCYCODE VARCHAR(50),
    CURRENCY VARCHAR(100),
    LANGUAGECODE VARCHAR(50),
    CATEGORY VARCHAR(50),
    CATEGORYNAME VARCHAR(150),
    CHARACTERISTICNAME VARCHAR(200),
    CHARACTERISTICDISPLAYNAME VARCHAR(200),
    STATISTICTYPE VARCHAR(50),
    CHARACTERISTICVALUE VARCHAR(100),
    ABBREVIATEDTEXT VARCHAR(500),
    HISTORYDATE DATE
)
"""
conn.cursor().execute(create_table_sql)

# Step 7: Upload metrics DataFrame to Snowflake
success, nchunks, nrows, _ = write_pandas(
    conn,
    df=final_df,
    table_name="BENCHMARKCHARACTERISTICS_NEW",
    quote_identifiers=True
)

# Close connection
conn.close()
print(f"Uploaded {nrows} rows to BENCHMARKCHARACTERISTICS_NEW.")

"""
Benchmark Characteristics Validation Script

This script validates the contents of the BENCHMARKCHARACTERISTICS_NEW
table after metrics have been calculated and uploaded. It checks for:
- Total row count
- Null values in key fields (e.g., BENCHMARKCODE, CHARACTERISTICVALUE)
- Extreme or invalid values (e.g., CHARACTERISTICVALUE outside ±100%)
- Duplicate rows based on BENCHMARKCODE, HISTORYDATE, and CHARACTERISTICNAME
- Missing HISTORYDATE values
- The overall date range of the data

All issues are logged using a named logger (not printed), to maintain
clean and professional diagnostics.
"""

import logging

# Configure a named logger for benchmark validation
logger = logging.getLogger("benchmark_validation")
logger.setLevel(logging.WARNING)

# Set up logging format and console handler
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter("WARNING:%(name)s: %(message)s"))
logger.handlers = [console_handler]  # Replace any default handlers

# Run post-upload validation on final_df
if not final_df.empty:
    # Log total number of rows
    logger.warning(f"Total rows: {len(final_df)}")

    # Count null BENCHMARKCODEs
    missing_benchmarkcode = final_df["BENCHMARKCODE"].isnull().sum()
    if missing_benchmarkcode > 0:
        logger.warning(f"Missing BENCHMARKCODE values: {missing_benchmarkcode}")

    # Count missing or null CHARACTERISTICVALUE entries
    missing_values = final_df["CHARACTERISTICVALUE"].isnull().sum()
    if missing_values > 0:
        logger.warning(f"Missing CHARACTERISTICVALUE values: {missing_values}")

    # Count extreme CHARACTERISTICVALUEs if numeric (outside ±100%)
    try:
        numeric_vals = pd.to_numeric(final_df["CHARACTERISTICVALUE"], errors="coerce")
        extreme_values = numeric_vals[(numeric_vals < -1) | (numeric_vals > 1)].count()
        if extreme_values > 0:
            logger.warning(f"Extreme value rows outside ±100%: {extreme_values}")
    except Exception as e:
        logger.warning(f"Failed to evaluate extreme value check: {e}")

    # Count duplicate entries
    duplicates = final_df.duplicated(
        subset=["BENCHMARKCODE", "HISTORYDATE", "CHARACTERISTICNAME"]
    ).sum()
    if duplicates > 0:
        logger.warning(f"Duplicate entries: {duplicates}")

    # Count null HISTORYDATEs
    missing_dates = final_df["HISTORYDATE"].isnull().sum()
    if missing_dates > 0:
        logger.warning(f"Missing dates: {missing_dates}")

    # Log date range
    if final_df["HISTORYDATE"].notnull().any():
        earliest_date = final_df["HISTORYDATE"].min()
        latest_date = final_df["HISTORYDATE"].max()
        logger.warning(f"Date range: {earliest_date} to {latest_date}")

