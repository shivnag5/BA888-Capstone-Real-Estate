# -*- coding: utf-8 -*-
"""PortfolioPerformance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EsLapQfKrbZa10tDvJIYoi0iqtQFvvjF
"""

"""
Historical Portfolio Performance Generator

This script pulls historical adjusted price data for each REIT portfolio based on
assigned tickers from the HOLDINGS_DETAILS_UPDATED metadata in Snowflake. It assigns
each ticker to one of five portfolios using the PRIMARYINDUSTRYNAME field, and only
includes tickers with at least 3 years of historical price data.

For each valid ticker, the script calculates daily portfolio performance using
market-cap-weighted returns. Three types of performance factors:
Portfolio Gross, Portfolio Net, and Model Net of Fees.

The final daily portfolio performance is uploaded to the Snowflake table
PORTFOLIOPERFORMANCE_NEW. If the table does not exist, it is created automatically.
"""

# Install required packages
!pip install --quiet snowflake-connector-python python-dotenv yfinance

import os
import pandas as pd
import yfinance as yf
import snowflake.connector
from dotenv import load_dotenv
from google.colab import files
from datetime import datetime, timedelta
from snowflake.connector.pandas_tools import write_pandas

# Step 1: Upload and load environment variables
uploaded = files.upload()
if os.path.exists(".env.txt"):
    os.rename(".env.txt", ".env")
elif os.path.exists(".env (2).txt"):
    os.rename(".env (2).txt", ".env")
load_dotenv()

# Step 2: Connect to Snowflake using credentials from .env
conn = snowflake.connector.connect(
    user=os.getenv("SNOWFLAKE_USER"),
    password=os.getenv("SNOWFLAKE_PASSWORD"),
    account=os.getenv("SNOWFLAKE_ACCOUNT"),
    warehouse=os.getenv("SNOWFLAKE_WAREHOUSE"),
    database="AST_REALESTATE_DB",
    schema="DBO",
    role=os.getenv("SNOWFLAKE_ROLE")
)

# Step 3: Pull tickers and industry metadata from HOLDINGS_DETAILS_UPDATED
query = """
    SELECT DISTINCT TICKER, PRIMARYINDUSTRYNAME
    FROM AST_REALESTATE_DB.DBO.HOLDINGS_DETAILS_UPDATED
    WHERE TICKER IS NOT NULL
"""
df_sec = pd.read_sql(query, conn)

# Step 4: Assign tickers to portfolios based on keywords in PRIMARYINDUSTRYNAME
portfolio_map = {
    "REITPORTFOLIO001": ["healthcare"],
    "REITPORTFOLIO002": ["services", "platform"],
    "REITPORTFOLIO003": ["diversified"],
    "REITPORTFOLIO004": ["mortgage", "hybrid"],
    "REITPORTFOLIO005": ["industrial", "retail", "infrastructure"]
}

def assign_portfolio(industry):
    if pd.isna(industry): return None
    industry = industry.lower()
    for code, keywords in portfolio_map.items():
        if any(k in industry for k in keywords):
            return code
    return None

df_sec["PORTFOLIOCODE"] = df_sec["PRIMARYINDUSTRYNAME"].apply(assign_portfolio)
assigned = df_sec[df_sec["PORTFOLIOCODE"].notna()]
portfolios = assigned.groupby("PORTFOLIOCODE")["TICKER"].unique().to_dict()

# Step 5: Define types of simulated performance
perf_types = {
    "Portfolio Gross": 0.0000,
    "Portfolio Net": -0.0001,
    "Model Net of Fees": -0.0002
}

perf_all = []
min_years = 3
cutoff = datetime.today() - timedelta(days=365 * min_years)

# Step 6: Loop through each portfolio and calculate returns
for pcode, tickers in portfolios.items():
    print(f"\nProcessing {pcode} with {len(tickers)} tickers...")
    valid = {}

    for t in tickers:
        try:
            ticker_obj = yf.Ticker(t)
            hist = ticker_obj.history(period="max", auto_adjust=True)

            if not hist.empty:
                first_date = pd.to_datetime(hist.index[0]).replace(tzinfo=None)
                if first_date <= cutoff:
                    valid[t] = hist["Close"]
                else:
                    print(f"Skipping {t} - insufficient history.")
            else:
                print(f"Skipping {t} - empty history.")
        except Exception as e:
            print(f"Skipping {t} - fetch error: {e}")

    price_df = pd.DataFrame(valid).dropna(how="all")
    inception = price_df.index.min()
    print(f"{pcode} inception: {inception.date()} | Valid tickers: {len(valid)}")

    # Step 7: Retrieve market caps for valid tickers
    mcaps = {}
    for t in valid:
        try:
            info = yf.Ticker(t).info
            mc = info.get("marketCap")
            if mc:
                mcaps[t] = mc
        except:
            continue

    final_tickers = [t for t in price_df.columns if t in mcaps]
    if not final_tickers:
        print(f"No tickers with market cap for {pcode}. Skipping.")
        continue

    price_df = price_df[final_tickers]
    weights = pd.Series(mcaps)[final_tickers]
    weights /= weights.sum()

    # Step 8: Compute daily portfolio returns
    daily_ret = price_df.pct_change(fill_method=None).dropna()
    port_ret = []
    for dt, row in daily_ret.iterrows():
        avail = row.dropna()
        w_today = weights[avail.index]
        w_today /= w_today.sum()
        r = (avail * w_today).sum()
        port_ret.append((dt, r))

    # Step 9: Format final results for Snowflake
    for ptype, fee_adj in perf_types.items():
        for dt, r in port_ret:
            perf_all.append({
                "PORTFOLIOCODE": pcode,
                "HISTORYDATE": dt.date(),
                "CURRENCYCODE": "USD",
                "CURRENCY": "US Dollar",
                "PERFORMANCECATEGORY": "Asset Class",
                "PERFORMANCECATEGORYNAME": "Total Portfolio",
                "PERFORMANCETYPE": ptype,
                "PERFORMANCEINCEPTIONDATE": inception.date(),
                "PORTFOLIOINCEPTIONDATE": inception.date(),
                "PERFORMANCEFREQUENCY": "D",
                "PERFORMANCEFACTOR": r + fee_adj
            })

# Step 10: Create target table in Snowflake if it doesn't exist
create_stmt = """
CREATE TABLE IF NOT EXISTS PORTFOLIOPERFORMANCE_NEW (
    PORTFOLIOCODE STRING,
    HISTORYDATE DATE,
    CURRENCYCODE STRING,
    CURRENCY STRING,
    PERFORMANCECATEGORY STRING,
    PERFORMANCECATEGORYNAME STRING,
    PERFORMANCETYPE STRING,
    PERFORMANCEINCEPTIONDATE DATE,
    PORTFOLIOINCEPTIONDATE DATE,
    PERFORMANCEFREQUENCY STRING,
    PERFORMANCEFACTOR FLOAT
)
"""
conn.cursor().execute(create_stmt)

# Step 11: Upload final performance DataFrame to Snowflake
perf_df = pd.DataFrame(perf_all)
success, nchunks, nrows, _ = write_pandas(conn, perf_df, "PORTFOLIOPERFORMANCE_NEW")
print(f"\nUpload success: {success} | Rows inserted: {nrows}")

# Step 12: Close Snowflake connection
conn.close()
print("Snowflake connection closed.")

import logging
import pandas as pd

# Configure logging to display warnings and info messages consistently
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


def validate_portfolio_performance(df: pd.DataFrame) -> pd.DataFrame:
    """
    Validate the portfolio performance DataFrame before uploading to Snowflake.

    This validator checks for:
    - Presence of required columns
    - Null values in key fields
    - Out-of-range performance factors
    - Duplicated rows based on key identifiers
    - Date range for HISTORYDATE column

    Parameters
    ----------
    df : pd.DataFrame
        The portfolio performance DataFrame to validate.

    Returns
    -------
    pd.DataFrame
        A single-row DataFrame summarizing validation metrics.
    """

    # Define required columns for performance table structure
    required_cols = [
        "PORTFOLIOCODE",
        "HISTORYDATE",
        "PERFORMANCEFACTOR",
        "PERFORMANCETYPE",
        "CURRENCYCODE",
        "PERFORMANCEFREQUENCY"
    ]

    # Check for any missing required columns
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        logging.error(f"Missing required columns: {missing_cols}")
        return pd.DataFrame()  # Return empty DataFrame if structure is invalid

    # Construct a dictionary summarizing all key validation checks
    summary = {
        "total_rows": len(df),
        "missing_portfolio_codes": df["PORTFOLIOCODE"].isnull().sum(),
        "missing_performance_values": df["PERFORMANCEFACTOR"].isnull().sum(),
        "negative_or_extreme_returns": df["PERFORMANCEFACTOR"].apply(
            lambda x: pd.isna(x) or x < -1 or x > 2
        ).sum(),  # Flag values outside normal daily return range
        "missing_dates": df["HISTORYDATE"].isnull().sum(),
        "duplicate_entries": df.duplicated(
            subset=["PORTFOLIOCODE", "HISTORYDATE", "PERFORMANCETYPE"]
        ).sum(),
        "earliest_date": df["HISTORYDATE"].min(),
        "latest_date": df["HISTORYDATE"].max()
    }

    # Log the total number of rows as an informational message
    logging.info(f"Total rows: {summary['total_rows']}")

    # Log warnings only for validation issues with nonzero counts
    for key, val in summary.items():
        if key != "total_rows" and isinstance(val, int) and val > 0:
            logging.warning(f"{key.replace('_', ' ').capitalize()}: {val}")

    # If no issues found in any integer metric, log a clean pass
    if not any(val > 0 for key, val in summary.items() if isinstance(val, int) and key != "total_rows"):
        logging.info("Portfolio performance validation passed with no issues.")

    return pd.DataFrame([summary])